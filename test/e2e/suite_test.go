package e2e

import (
	"context"
	"encoding/json"
	"flag"
	"fmt"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"strings"
	"testing"
	"time"

	"github.com/k8ssandra/cass-operator/apis/control/v1alpha1"

	"github.com/k8ssandra/k8ssandra-operator/apis/config/v1beta1"

	reaperclient "github.com/k8ssandra/reaper-client-go/reaper"
	"gopkg.in/resty.v1"

	"github.com/k8ssandra/k8ssandra-operator/pkg/cassandra"
	"github.com/k8ssandra/k8ssandra-operator/pkg/telemetry"
	"github.com/k8ssandra/k8ssandra-operator/pkg/utils"

	"github.com/k8ssandra/k8ssandra-operator/pkg/annotations"
	"github.com/k8ssandra/k8ssandra-operator/pkg/stargate"

	"github.com/k8ssandra/k8ssandra-operator/test/kustomize"
	"github.com/rs/zerolog"
	"github.com/rs/zerolog/log"
	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/utils/pointer"
	"sigs.k8s.io/controller-runtime/pkg/client"

	cassdcapi "github.com/k8ssandra/cass-operator/apis/cassandra/v1beta1"
	api "github.com/k8ssandra/k8ssandra-operator/apis/k8ssandra/v1alpha1"
	reaperapi "github.com/k8ssandra/k8ssandra-operator/apis/reaper/v1alpha1"
	stargateapi "github.com/k8ssandra/k8ssandra-operator/apis/stargate/v1alpha1"
	"github.com/k8ssandra/k8ssandra-operator/test/framework"
	"github.com/k8ssandra/k8ssandra-operator/test/kubectl"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	appsv1 "k8s.io/api/apps/v1"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/apimachinery/pkg/util/rand"
)

type pollingConfig struct {
	timeout  time.Duration
	interval time.Duration
}

type ingressConfig struct {
	StargateRest framework.HostAndPort `json:"stargate_rest"`
	StargateCql  framework.HostAndPort `json:"stargate_cql"`
	StargateGrpc framework.HostAndPort `json:"stargate_grpc"`
	ReaperRest   framework.HostAndPort `json:"reaper_rest"`
	Solr         framework.HostAndPort `json:"solr"`
	Graph        framework.HostAndPort `json:"graph"`
}

var (
	polling struct {
		nodetoolStatus           pollingConfig
		datacenterReady          pollingConfig
		operatorDeploymentReady  pollingConfig
		k8ssandraClusterStatus   pollingConfig
		stargateReady            pollingConfig
		reaperReady              pollingConfig
		medusaReady              pollingConfig
		medusaBackupDone         pollingConfig
		medusaRestoreDone        pollingConfig
		datacenterUpdating       pollingConfig
		cassandraTaskCreated     pollingConfig
		medusaConfigurationReady pollingConfig
	}
)

var (
	kubeconfigFileFlag = flag.String(
		"kubeconfigFile",
		"./build/kind-kubeconfig",
		"The path to the kubeconfig file to use to create remote clients and ClientConfigs. "+
			"The default path points to the kubeconfig file generated by setup-kind-multicluster.sh. "+
			"The path must be either absolute, or relative to the k8ssandra-operator repository root.",
	)
	controlPlaneFlag = flag.String(
		"controlPlane",
		"kind-k8ssandra-0",
		"The name of a Kubernetes context to use as the control plane context.",
	)
	dataPlanesFlag = flag.String(
		"dataPlanes",
		"kind-k8ssandra-0,kind-k8ssandra-1",
		"A comma-separated list of Kubernetes context names to use as data planes. "+
			"Must contain at least one data plane. "+
			"The control plane should not be included here, unless it is also a data plane.",
	)
	externalIPsFlag = flag.String(
		"externalIPs",
		"",
		"A comma-separated list of external IPs for each data plane. "+
			"If empty, localhost will be used and port numbers will be adapted according to the setup "+
			"generated by setup-kind-multicluster.sh.",
	)
	zoneMappingsFlag = flag.String(
		"zoneMappings",
		`{
					"region1-zone1" : "region1-zone1",
					"region1-zone2" : "region1-zone2",
					"region1-zone3" : "region1-zone3",
					"region2-zone1" : "region2-zone1",
					"region2-zone2" : "region2-zone2",
					"region2-zone3" : "region2-zone3",
					"region3-zone1" : "region3-zone1",
					"region3-zone2" : "region3-zone2",
					"region3-zone3" : "region3-zone3"
  				}`,
		"A JSON string containing zone mappings for each rack present in the fixtures. "+
			"The default mapping is idempotent; it is suitable for use with local Kind clusters "+
			"generated by setup-kind-multicluster.sh.",
	)
	storageClassFlag = flag.String(
		"storage",
		"standard",
		"The name of a storage class to use for persistent volumes. "+
			"The default is suitable for use with local Kind clusters "+
			"generated by setup-kind-multicluster.sh.",
	)
	hostNetworkFlag = flag.Bool(
		"hostNetwork",
		true,
		"Whether to use host networking for Cassandra and Stargate pods. "+
			"This is only required for Kind clusters and should be set to false if the cluster is running in a cloud provider.",
	)
	logKustomizeOutput = flag.Bool(
		"logKustomizeOutput",
		false,
		"Whether output of kustomize should be logged.",
	)
	logKubectlOutput = flag.Bool(
		"logKubectlOutput",
		false,
		"Whether output of kubectl should be logged.",
	)
	imageName = flag.String(
		"imageName",
		"k8ssandra/k8ssandra-operator",
		"The k8ssandra-operator image name to use.",
	)
	imageTag = flag.String(
		"imageTag",
		"latest",
		"The k8ssandra-operator image tag to use.",
	)
	medusaImageTag = flag.String(
		"medusaImageTag",
		"",
		"The medusa image tag to use.",
	)
)

var (
	kubeconfigFile string
	controlPlane   string
	dataPlanes     []string
	ingressConfigs map[string]ingressConfig
	zoneMappings   map[string]string
)

func TestOperator(t *testing.T) {
	beforeSuite(t)

	ctx := context.Background()

	t.Run("CreateSingleDatacenterCluster", e2eTest(ctx, &e2eTestOpts{
		testFunc: createSingleDatacenterCluster,
		fixture:  framework.NewTestFixture("single-dc", controlPlane),
	}))
	t.Run("CreateSingleDseDatacenterCluster", e2eTest(ctx, &e2eTestOpts{
		testFunc: createSingleDseDatacenterCluster,
		fixture:  framework.NewTestFixture("single-dc-dse", controlPlane),
		dse:      true,
	}))
	t.Run("CreateSingleDseSearchDatacenterCluster", e2eTest(ctx, &e2eTestOpts{
		testFunc:     createSingleDseSearchDatacenterCluster,
		fixture:      framework.NewTestFixture("single-dc-dse-search", controlPlane),
		dse:          true,
		installMinio: true,
	}))
	t.Run("CreateSingleDseGraphDatacenterCluster", e2eTest(ctx, &e2eTestOpts{
		testFunc: createSingleDseGraphDatacenterCluster,
		fixture:  framework.NewTestFixture("single-dc-dse-graph", controlPlane),
		dse:      true,
	}))
	t.Run("ChangeDseWorkload", e2eTest(ctx, &e2eTestOpts{
		testFunc: changeDseWorkload,
		fixture:  framework.NewTestFixture("single-dc-dse", controlPlane),
		dse:      true,
	}))
	t.Run("CreateStargateAndDatacenter", e2eTest(ctx, &e2eTestOpts{
		testFunc:                     createStargateAndDatacenter,
		fixture:                      framework.NewTestFixture("stargate", dataPlanes[0]),
		skipK8ssandraClusterCleanup:  true,
		doCassandraDatacenterCleanup: true,
	}))
	t.Run("CreateMultiDatacenterCluster", e2eTest(ctx, &e2eTestOpts{
		testFunc: createMultiDatacenterCluster,
		fixture:  framework.NewTestFixture("multi-dc", controlPlane),
	}))
	t.Run("CreateMultiDatacenterDseCluster", e2eTest(ctx, &e2eTestOpts{
		testFunc: createMultiDatacenterDseCluster,
		fixture:  framework.NewTestFixture("multi-dc-dse", controlPlane),
	}))
	t.Run("CreateMixedMultiDataCenterCluster", e2eTest(ctx, &e2eTestOpts{
		testFunc: createMultiDatacenterClusterDifferentTopologies,
		fixture:  framework.NewTestFixture("multi-dc-mixed", controlPlane),
	}))
	t.Run("AddDcToClusterDiffDataplane", e2eTest(ctx, &e2eTestOpts{
		testFunc: addDcToCluster,
		fixture:  framework.NewTestFixture("add-dc", controlPlane),
	}))
	t.Run("AddDcToClusterSameDataplane", e2eTest(ctx, &e2eTestOpts{
		testFunc:             addDcToClusterSameDataplane,
		fixture:              framework.NewTestFixture("add-dc-cass-only", controlPlane),
		clusterScoped:        true,
		additionalNamespaces: []string{"dc2"},
		sutNamespace:         "k8ssandra-operator",
	}))
	t.Run("RemoveDcFromCluster", e2eTest(ctx, &e2eTestOpts{
		testFunc: removeDcFromCluster,
		fixture:  framework.NewTestFixture("remove-dc", controlPlane),
	}))
	t.Run("RemoveLocalDcFromCluster", e2eTest(ctx, &e2eTestOpts{
		testFunc: removeLocalDcFromCluster,
		fixture:  framework.NewTestFixture("remove-local-dc", controlPlane),
	}))
	t.Run("CreateMultiStargateAndDatacenter", e2eTest(ctx, &e2eTestOpts{
		testFunc:                     createStargateAndDatacenter,
		fixture:                      framework.NewTestFixture("multi-stargate", dataPlanes[0]),
		skipK8ssandraClusterCleanup:  true,
		doCassandraDatacenterCleanup: true,
	}))
	t.Run("CheckStargateApisWithMultiDcCluster", e2eTest(ctx, &e2eTestOpts{
		testFunc: checkStargateApisWithMultiDcCluster,
		fixture:  framework.NewTestFixture("multi-dc-stargate", controlPlane),
	}))
	t.Run("CreateSingleReaperNoStargate", e2eTest(ctx, &e2eTestOpts{
		testFunc:                     createSingleReaper,
		fixture:                      framework.NewTestFixture("single-dc-reaper", controlPlane),
		skipK8ssandraClusterCleanup:  false,
		doCassandraDatacenterCleanup: true,
	}))
	t.Run("CreateSingleReaperWStargateAndHTTP", e2eTest(ctx, &e2eTestOpts{
		testFunc:                     createSingleReaper,
		fixture:                      framework.NewTestFixture("stargate-reaper-http", controlPlane),
		skipK8ssandraClusterCleanup:  false,
		doCassandraDatacenterCleanup: true,
	}))
	t.Run("CreateMultiReaper", e2eTest(ctx, &e2eTestOpts{
		testFunc: createMultiReaper,
		fixture:  framework.NewTestFixture("multi-dc-reaper", controlPlane),
	}))
	t.Run("CreateReaperAndDatacenter", e2eTest(ctx, &e2eTestOpts{
		testFunc:                     createReaperAndDatacenter,
		fixture:                      framework.NewTestFixture("reaper", dataPlanes[0]),
		skipK8ssandraClusterCleanup:  true,
		doCassandraDatacenterCleanup: true,
	}))
	t.Run("ClusterScoped", func(t *testing.T) {
		t.Run("MultiDcMultiCluster", e2eTest(ctx, &e2eTestOpts{
			testFunc:             multiDcMultiCluster,
			fixture:              framework.NewTestFixture("multi-dc-cluster-scope", controlPlane),
			clusterScoped:        true,
			sutNamespace:         "test-0",
			additionalNamespaces: []string{"test-1", "test-2"},
			installMinio:         true,
		}))
	})
	t.Run("CreateSingleMedusaJob", e2eTest(ctx, &e2eTestOpts{
		testFunc:                     createSingleMedusaJob,
		fixture:                      framework.NewTestFixture("single-dc-encryption-medusa", controlPlane),
		skipK8ssandraClusterCleanup:  false,
		doCassandraDatacenterCleanup: false,
		installMinio:                 true,
	}))
	t.Run("CreateMultiDcSingleMedusaJob", e2eTest(ctx, &e2eTestOpts{
		testFunc:                     createMultiDcSingleMedusaJob,
		fixture:                      framework.NewTestFixture("single-dc-multi-cluster-medusa", controlPlane),
		skipK8ssandraClusterCleanup:  false,
		doCassandraDatacenterCleanup: false,
		installMinio:                 true,
	}))
	t.Run("CreateSingleDseMedusaJob", e2eTest(ctx, &e2eTestOpts{
		testFunc:                     createSingleMedusaJob,
		fixture:                      framework.NewTestFixture("single-dc-dse-medusa", controlPlane),
		skipK8ssandraClusterCleanup:  false,
		doCassandraDatacenterCleanup: false,
		installMinio:                 true,
	}))
	t.Run("CreateMultiMedusaJob", e2eTest(ctx, &e2eTestOpts{
		testFunc:                     createMultiMedusaJob,
		fixture:                      framework.NewTestFixture("multi-dc-encryption-medusa", controlPlane),
		skipK8ssandraClusterCleanup:  false,
		doCassandraDatacenterCleanup: false,
		installMinio:                 true,
	}))
	t.Run("MultiDcAuthOnOff", e2eTest(ctx, &e2eTestOpts{
		testFunc: multiDcAuthOnOff,
		fixture:  framework.NewTestFixture("multi-dc-auth", controlPlane),
	}))
	t.Run("ConfigControllerRestarts", e2eTest(ctx, &e2eTestOpts{
		testFunc:                    controllerRestart,
		skipK8ssandraClusterCleanup: true,
	}))
	t.Run("SingleDcEncryptionWithStargate", e2eTest(ctx, &e2eTestOpts{
		testFunc: createSingleDatacenterClusterWithEncryption,
		fixture:  framework.NewTestFixture("single-dc-encryption-stargate", controlPlane),
	}))
	t.Run("SingleDcEncryptionWithReaper", e2eTest(ctx, &e2eTestOpts{
		testFunc: createSingleReaperWithEncryption,
		fixture:  framework.NewTestFixture("single-dc-encryption-reaper", controlPlane),
	}))
	t.Run("MultiDcEncryptionWithStargate", e2eTest(ctx, &e2eTestOpts{
		testFunc: checkStargateApisWithMultiDcEncryptedCluster,
		fixture:  framework.NewTestFixture("multi-dc-encryption-stargate", controlPlane),
	}))
	t.Run("MultiDcEncryptionWithReaper", e2eTest(ctx, &e2eTestOpts{
		testFunc: createMultiReaperWithEncryption,
		fixture:  framework.NewTestFixture("multi-dc-encryption-reaper", controlPlane),
	}))
	t.Run("StopAndRestartDc", e2eTest(ctx, &e2eTestOpts{
		testFunc: stopAndRestartDc,
		fixture:  framework.NewTestFixture("stop-dc", controlPlane),
	}))
	t.Run("GCTests", func(t *testing.T) {
		t.Run("3.11-jdk8-G1", e2eTest(ctx, &e2eTestOpts{
			testFunc: gcTest("G1"),
			fixture:  framework.NewTestFixture("gc/3.11-jdk8-G1", controlPlane),
		}))
		t.Run("3.11-jdk8-CMS", e2eTest(ctx, &e2eTestOpts{
			testFunc: gcTest("CMS"),
			fixture:  framework.NewTestFixture("gc/3.11-jdk8-CMS", controlPlane),
		}))
		t.Run("4.0-jdk11-G1", e2eTest(ctx, &e2eTestOpts{
			testFunc: gcTest("G1"),
			fixture:  framework.NewTestFixture("gc/4.0-jdk11-G1", controlPlane),
		}))
		t.Run("4.0-jdk11-CMS", e2eTest(ctx, &e2eTestOpts{
			testFunc: gcTest("CMS"),
			fixture:  framework.NewTestFixture("gc/4.0-jdk11-CMS", controlPlane),
		}))
		t.Run("4.0-jdk11-ZGC", e2eTest(ctx, &e2eTestOpts{
			testFunc: gcTest("ZGC"),
			fixture:  framework.NewTestFixture("gc/4.0-jdk11-ZGC", controlPlane),
		}))
	})
	t.Run("UpgradeOperatorImage", e2eTest(ctx, &e2eTestOpts{
		testFunc:       createSingleDatacenterClusterWithUpgrade,
		fixture:        framework.NewTestFixture("single-dc-upgrade", controlPlane),
		initialVersion: pointer.String("v1.4.1"), // Has to be the Helm chart version, not the operator image tag
	}))
	t.Run("StargateJwt", e2eTest(ctx, &e2eTestOpts{
		testFunc: stargateJwt,
		fixture:  framework.NewTestFixture("stargate-jwt", controlPlane),
	}))
	t.Run("PerNodeConfig", func(t *testing.T) {
		t.Run("InitialTokens", e2eTest(ctx, &e2eTestOpts{
			testFunc: multiDcInitialTokens,
			fixture:  framework.NewTestFixture("multi-dc-initial-tokens", controlPlane),
		}))
		t.Run("UserDefined", e2eTest(ctx, &e2eTestOpts{
			testFunc: userDefinedPerNodeConfig,
			fixture:  framework.NewTestFixture("single-dc-per-node-config", controlPlane),
		}))
	})
	t.Run("CreateMultiDatacenterTask", e2eTest(ctx, &e2eTestOpts{
		testFunc: createMultiDatacenterTask,
		fixture:  framework.NewTestFixture("multi-dc", controlPlane),
	}))
	t.Run("CreateMedusaConfiguration", e2eTest(ctx, &e2eTestOpts{
		testFunc: createMedusaConfiguration,
		fixture:  framework.NewTestFixture("medusa-configuration", controlPlane),
	}))
}

func beforeSuite(t *testing.T) {

	processFlags(t)
	applyPollingDefaults()

	kustomize.LogOutput(*logKustomizeOutput)
	kubectl.LogOutput(*logKubectlOutput)

	// TODO this needs to go away since we are create a Framework instance per test now
	framework.Init(t)

	// Used by the go-cassandra-native-protocol library
	configureZeroLog()
}

// e2eTestOpts configures an e2e test for execution.
type e2eTestOpts struct {
	// testFunc is the test function to be executed.
	testFunc e2eTestFunc

	// fixture specifies the fixture tu use.
	fixture *framework.TestFixture

	// clusterScoped specifies whether the operator is configured to watch all namespaces.
	clusterScoped bool

	// operatorNamespace is the namespace in which k8ssandra-operator is deployed. When the
	// operator is configured to only watch a single namespace, the test framework will
	// configure this to be the same as sutNamespace. When the operator is configured
	// to watch multiple namespaces, the test framework will configure this to be
	// k8ssandra-operator.
	operatorNamespace string

	// sutNamespace is the namespace in which the system under test (typically a
	// K8ssandraCluster) is deployed. When the operator is configured to watch a single
	// namespace, it will automatically set based on the fixture name. When the operator
	// is cluster-scoped, this needs to be explicitly set.
	sutNamespace string

	// additionalNamespaces provides an optional set of namespaces for use in tests where
	// the operator is cluster-scoped. For example, the K8ssandraCluster and each of its
	// CassandraDatacenters can be deployed in different namespaces. The K8ssandraCluster
	// namespace should be specified by sutNamespace and the CassandraDatacenter namespaces
	// should be specified here.
	additionalNamespaces []string

	// skipK8ssandraClusterCleanup is a flag that lets the framework know if deleting the
	// K8ssandraCluster should be skipped as would be the case for test that only involve
	// other components like Stargate and Reaper.
	skipK8ssandraClusterCleanup bool

	// doCassandraDatacenterCleanup is a flag that lets the framework know it should perform
	// deletions of CassandraDatacenters that are not part of a K8ssandraCluster.
	doCassandraDatacenterCleanup bool

	// initialVersion is used to set the initial version of the operator when performing
	// an upgrade test.
	initialVersion *string

	// dse is used to specify if the e2e tests will run against DSE or Cassandra
	dse bool

	// installMinio is used to specify if the e2e tests will require to install Minio before creating the k8c object.
	installMinio bool
}

type e2eTestFunc func(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework)

func e2eTest(ctx context.Context, opts *e2eTestOpts) func(*testing.T) {
	return func(t *testing.T) {

		f, err := framework.NewE2eFramework(t, kubeconfigFile, opts.dse, controlPlane, dataPlanes...)
		if err != nil {
			t.Fatalf("failed to initialize test framework: %v", err)
		}

		setTestNamespaceNames(opts)

		err = beforeTest(t, f, opts)
		defer afterTest(t, f, opts)

		if err == nil {
			opts.testFunc(t, ctx, opts.sutNamespace, f)
		} else {
			t.Errorf("before test setup failed: %v", err)
		}
	}
}

// setTestNamespaceNames initializes the operatorNamespace and sutNamespace fields. When
// the operator is cluster-scoped, it is always deployed in the k8ssandra-operator
// namespace. sutNamespace is not set because the K8ssadraCluster can be deployed in any
// namespace. When the operator is namespace-scoped both operatorNamespace and sutNamespace
// are set to the same value which is the fixture name plus a random suffix.
func setTestNamespaceNames(opts *e2eTestOpts) {
	if opts.clusterScoped {
		opts.operatorNamespace = "k8ssandra-operator"
	} else {
		if opts.fixture != nil {
			opts.operatorNamespace = framework.CleanupForKubernetes(opts.fixture.Name + "-" + rand.String(6))
		} else {
			opts.operatorNamespace = framework.CleanupForKubernetes(rand.String(9))
		}
		opts.sutNamespace = opts.operatorNamespace
	}
}

// beforeTest Creates the test namespace, deploys k8ssandra-operator, and then deploys the
// test fixture. Deploying k8ssandra-operator includes cass-operator and all of the CRDs
// required by both operators.
func beforeTest(t *testing.T, f *framework.E2eFramework, opts *e2eTestOpts) error {
	namespaces := make([]string, 0)

	if opts.clusterScoped {
		namespaces = append(namespaces, opts.sutNamespace)
	}

	namespaces = append(namespaces, opts.operatorNamespace)

	if len(opts.additionalNamespaces) > 0 {
		namespaces = append(namespaces, opts.additionalNamespaces...)
	}

	if opts.installMinio {
		namespaces = append(namespaces, framework.MinioNamespace)
	}

	for _, namespace := range namespaces {
		if err := f.CreateNamespace(namespace); err != nil {
			t.Logf("failed to create namespace %s", namespace)
			return err
		}

		if err := f.DeployCassandraConfigMap(namespace); err != nil {
			t.Log("failed to deploy cassandra configmap")
			return err
		}
	}

	deploymentConfig := framework.OperatorDeploymentConfig{
		Namespace:           opts.operatorNamespace,
		ClusterScoped:       opts.clusterScoped,
		ImageName:           *imageName,
		ImageTag:            *imageTag,
		MedusaImageTag:      *medusaImageTag,
		GithubKustomization: false,
	}

	if opts.initialVersion != nil {
		deploymentConfig.ImageTag = *opts.initialVersion
		deploymentConfig.GithubKustomization = true
	}

	if opts.installMinio {
		if err := f.CreateNamespace("minio"); err != nil {
			t.Logf("failed to create namespace %s", "minio")
			return err
		}
		if err := f.InstallMinio(); err != nil {
			t.Log("failed to install Minio operator")
			return err
		}
		if err := f.CreateMedusaBucket(framework.MinioNamespace); err != nil {
			t.Log("failed to create Medusa bucket")
			return err
		}
		if err := f.CreateMedusaSecret(opts.sutNamespace); err != nil {
			t.Log("failed to create Medusa secret")
			return err
		}
	}

	if err := f.DeployK8ssandraOperator(deploymentConfig); err != nil {
		t.Logf("failed to deploy k8ssandra-operator")
		return err
	}

	if err := f.WaitForCrdsToBecomeActive(); err != nil {
		t.Log("failed waiting for CRDs to become active")
		return err
	}

	if err := f.DeployK8sClientConfigs(opts.operatorNamespace, kubeconfigFile, kubeconfigFile, controlPlane); err != nil {
		t.Logf("failed to deploy client configs")
		return err
	}

	// Kill K8ssandraOperator pod to cause restart and load the client configs
	if err := f.DeleteK8ssandraOperatorPods(opts.operatorNamespace, polling.operatorDeploymentReady.timeout, polling.operatorDeploymentReady.interval); err != nil {
		t.Logf("failed to restart k8ssandra-operator")
		return err
	}

	if err := f.WaitForCassOperatorToBeReady(opts.operatorNamespace, polling.operatorDeploymentReady.timeout, polling.operatorDeploymentReady.interval); err != nil {
		t.Log("failed waiting for cass-operator to be ready")
		return err
	}

	if err := f.WaitForK8ssandraOperatorToBeReady(opts.operatorNamespace, polling.operatorDeploymentReady.timeout, polling.operatorDeploymentReady.interval); err != nil {
		t.Log("failed waiting for k8ssandra-operator to be ready")
		return err
	}

	if opts.fixture != nil {
		if err := f.DeployFixture(opts.sutNamespace, opts.fixture, zoneMappings, *storageClassFlag, *hostNetworkFlag, deploymentConfig.MedusaImageTag); err != nil {
			t.Logf("failed to deploy fixture")
			return err
		}
	}

	return nil
}

func upgradeToLatest(t *testing.T, ctx context.Context, f *framework.E2eFramework, namespace string) error {
	deploymentConfig := framework.OperatorDeploymentConfig{
		Namespace:     namespace,
		ClusterScoped: false,
		ImageName:     *imageName,
		ImageTag:      *imageTag,
	}

	if err := f.DeployK8ssandraOperator(deploymentConfig); err != nil {
		t.Logf("failed to deploy k8ssandra-operator")
		return err
	}

	if err := f.WaitForK8ssandraOperatorToBeReady(namespace, polling.operatorDeploymentReady.timeout, polling.operatorDeploymentReady.interval); err != nil {
		t.Log("failed waiting for k8ssandra-operator to be ready")
		return err
	}

	if err := f.WaitForCassOperatorToBeReady(namespace, polling.operatorDeploymentReady.timeout, polling.operatorDeploymentReady.interval); err != nil {
		t.Log("failed waiting for cass-operator to be ready")
		return err
	}

	dcKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc1"}}
	checkDatacenterUpdating(t, ctx, dcKey, f)
	checkDatacenterReady(t, ctx, dcKey, f)

	return nil
}

func processFlags(t *testing.T) {
	if len(*kubeconfigFileFlag) == 0 {
		t.Fatal("no kubeconfig file provided")
	}
	var err error
	if filepath.IsAbs(*kubeconfigFileFlag) {
		kubeconfigFile = *kubeconfigFileFlag
	} else {
		kubeconfigFile, err = filepath.Abs(filepath.Join("..", "..", *kubeconfigFileFlag))
	}
	if err == nil {
		_, err = os.Stat(kubeconfigFile)
	}
	if err != nil {
		t.Fatalf("invalid kubeconfig file: %s: %v", *kubeconfigFileFlag, err)
	}
	if len(*controlPlaneFlag) == 0 {
		t.Fatal("no control plane provided")
	}
	controlPlane = *controlPlaneFlag
	dataPlanes = strings.Split(*dataPlanesFlag, ",")
	if len(dataPlanes) == 0 {
		t.Fatal("no data planes provided")
	}
	ingressConfigs = make(map[string]ingressConfig)
	if *externalIPsFlag == "" {
		for _, name := range dataPlanes {
			prefix := name[strings.LastIndex(name, "-")+1:]
			ingressConfigs[name] = ingressConfig{
				StargateRest: framework.HostAndPort(fmt.Sprintf("stargate.127.0.0.1.nip.io:3%v080", prefix)),
				StargateCql:  framework.HostAndPort(fmt.Sprintf("stargate.127.0.0.1.nip.io:3%v942", prefix)),
				StargateGrpc: framework.HostAndPort(fmt.Sprintf("stargate-grpc.127.0.0.1.nip.io:3%v443", prefix)),
				ReaperRest:   framework.HostAndPort(fmt.Sprintf("reaper.127.0.0.1.nip.io:3%v080", prefix)),
				Solr:         framework.HostAndPort(fmt.Sprintf("solr.127.0.0.1.nip.io:3%v080", prefix)),
				Graph:        framework.HostAndPort(fmt.Sprintf("graph.127.0.0.1.nip.io:3%v080", prefix)),
			}
		}
	} else {
		ips := strings.Split(*externalIPsFlag, ",")
		if len(dataPlanes) != len(ips) {
			t.Fatal("external IPs provided do not match the number of data planes")
		}
		t.Logf("external IPs: %v", ips)
		for i, name := range dataPlanes {
			ip := ips[i]
			ingressConfigs[name] = ingressConfig{
				StargateRest: framework.HostAndPort(fmt.Sprintf("stargate.%v.nip.io:8080", ip)),
				StargateCql:  framework.HostAndPort(fmt.Sprintf("stargate.%v.nip.io:9042", ip)),
				StargateGrpc: framework.HostAndPort(fmt.Sprintf("stargate-grpc.%v.nip.io:8090", ip)),
				ReaperRest:   framework.HostAndPort(fmt.Sprintf("reaper.%v.nip.io:8080", ip)),
				Solr:         framework.HostAndPort(fmt.Sprintf("solr.%v.nip.io:8983", ip)),
			}
		}
	}
	err = json.Unmarshal([]byte(*zoneMappingsFlag), &zoneMappings)
	if err != nil {
		t.Fatalf("invalid racks json: %s: %v", *zoneMappingsFlag, err)
	}
}

func applyPollingDefaults() {
	polling.operatorDeploymentReady.timeout = 3 * time.Minute
	polling.operatorDeploymentReady.interval = 1 * time.Second

	polling.datacenterReady.timeout = 20 * time.Minute
	polling.datacenterReady.interval = 15 * time.Second

	polling.nodetoolStatus.timeout = 2 * time.Minute
	polling.nodetoolStatus.interval = 5 * time.Second

	polling.k8ssandraClusterStatus.timeout = 1 * time.Minute
	polling.k8ssandraClusterStatus.interval = 3 * time.Second

	polling.stargateReady.timeout = 5 * time.Minute
	polling.stargateReady.interval = 5 * time.Second

	polling.reaperReady.timeout = 10 * time.Minute
	polling.reaperReady.interval = 15 * time.Second

	polling.medusaBackupDone.timeout = 10 * time.Minute
	polling.medusaBackupDone.interval = 15 * time.Second

	polling.medusaRestoreDone.timeout = 10 * time.Minute
	polling.medusaRestoreDone.interval = 15 * time.Second

	polling.datacenterUpdating.timeout = 1 * time.Minute
	polling.datacenterUpdating.interval = 1 * time.Second

	polling.cassandraTaskCreated.timeout = 1 * time.Minute
	polling.cassandraTaskCreated.interval = 3 * time.Second

	polling.medusaReady.timeout = 5 * time.Minute
	polling.medusaReady.interval = 5 * time.Second

	polling.medusaConfigurationReady.timeout = 1 * time.Minute
	polling.medusaConfigurationReady.interval = 5 * time.Second
}

func afterTest(t *testing.T, f *framework.E2eFramework, opts *e2eTestOpts) {
	assert.NoError(t, cleanUp(t, f, opts), "after test cleanup failed")
}

func cleanUp(t *testing.T, f *framework.E2eFramework, opts *e2eTestOpts) error {
	namespaces := make([]string, 0)
	namespaces = append(namespaces, opts.operatorNamespace)
	if !utils.SliceContains(namespaces, opts.sutNamespace) {
		namespaces = append(namespaces, opts.sutNamespace)
	}
	if len(opts.additionalNamespaces) > 0 {
		namespaces = append(namespaces, opts.additionalNamespaces...)
	}

	if err := f.DumpClusterInfo(t.Name(), namespaces...); err != nil {
		t.Logf("failed to dump cluster info: %v", err)
	}

	timeout := 10 * time.Minute
	interval := 15 * time.Second

	if !opts.skipK8ssandraClusterCleanup {
		if err := f.DeleteK8ssandraClusters(opts.sutNamespace, timeout, interval); err != nil {
			t.Logf("failed to delete K8sandraCluster: %v", err)
			return err
		}
	}

	if opts.doCassandraDatacenterCleanup {
		if err := f.DeleteCassandraDatacenters(opts.sutNamespace, timeout, interval); err != nil {
			t.Logf("failed to delete CassandraDatacenter: %v", err)
		}
	}

	for _, namespace := range namespaces {
		if err := f.DeleteNamespace(namespace, timeout, interval); err != nil {
			t.Logf("failed to delete namespace: %v", err)
		}
	}

	return nil
}

// createSingleDatacenterCluster creates a K8ssandraCluster with one CassandraDatacenter
// and one Stargate node that are deployed in the local cluster.
func createSingleDatacenterCluster(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {
	require := require.New(t)

	t.Log("check that the K8ssandraCluster was created")
	k8ssandra := &api.K8ssandraCluster{}
	kcKey := types.NamespacedName{Namespace: namespace, Name: "test"}
	err := f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)

	require.Eventually(func() bool {
		err := f.Client.Get(ctx, kcKey, k8ssandra)
		if err != nil {
			return false
		}
		return "{\"dc1\":3}" == k8ssandra.ObjectMeta.Annotations["k8ssandra.io/initial-system-replication"]
	}, polling.k8ssandraClusterStatus.timeout, polling.k8ssandraClusterStatus.interval, "initial-system-replication annotation not set correctly according to dc name override")

	dcKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc1"}}
	checkDatacenterReady(t, ctx, dcKey, f)
	require.NoError(CheckLabelsAnnotationsCreated(dcKey, t, ctx, f))
	// Check that the Cassandra cluster name override is passed to the cassdc without being modified
	checkCassandraClusterName(t, ctx, k8ssandra, dcKey, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dcKey.Name)
	dcPrefix := DcPrefix(t, f, dcKey)
	require.NoError(checkMetricsFiltersAbsence(t, ctx, f, dcKey))
	require.NoError(checkInjectedContainersPresence(t, ctx, f, dcKey))
	require.NoError(checkInjectedVolumePresence(t, ctx, f, dcKey, 4))

	// check that the Cassandra Vector container and config map exist
	checkVectorAgentConfigMapPresence(t, ctx, f, dcKey, telemetry.VectorAgentConfigMapName)

	stargateKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: dcPrefix + "-stargate"}}
	checkStargateReady(t, f, ctx, stargateKey)

	checkStargateK8cStatusReady(t, f, ctx, kcKey, dcKey)

	// check that the Stargate Vector container and config map exist
	stargateDeploymentKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: dcPrefix + "-default-stargate-deployment"}}
	checkContainerPresence(t, ctx, f, stargateDeploymentKey, getPodTemplateSpecForDeployment, stargate.VectorContainerName)
	checkVectorAgentConfigMapPresence(t, ctx, f, dcKey, stargate.VectorAgentConfigMapName)

	t.Logf("check that if Stargate Vector is disabled, the agent and configmap are deleted")
	err = f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)
	stargateVectorPatch := client.MergeFromWithOptions(k8ssandra.DeepCopy(), client.MergeFromWithOptimisticLock{})
	k8ssandra.Spec.Cassandra.Datacenters[0].Stargate.Telemetry.Vector.Enabled = pointer.Bool(false)
	err = f.Client.Patch(ctx, k8ssandra, stargateVectorPatch)
	require.NoError(err, "failed to patch K8ssandraCluster in namespace %s", namespace)
	checkStargateReady(t, f, ctx, stargateKey)
	checkStargateK8cStatusReady(t, f, ctx, kcKey, dcKey)
	checkContainerDeleted(t, ctx, f, stargateDeploymentKey, getPodTemplateSpecForDeployment, stargate.VectorContainerName)
	checkVectorConfigMapDeleted(t, ctx, f, dcKey, stargate.VectorAgentConfigMapName)

	t.Logf("check that if Stargate Vector is enabled, the agent and configmap are re-created")
	err = f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)
	stargateVectorPatch = client.MergeFromWithOptions(k8ssandra.DeepCopy(), client.MergeFromWithOptimisticLock{})
	k8ssandra.Spec.Cassandra.Datacenters[0].Stargate.Telemetry.Vector.Enabled = pointer.Bool(true)
	err = f.Client.Patch(ctx, k8ssandra, stargateVectorPatch)
	require.NoError(err, "failed to patch K8ssandraCluster in namespace %s", namespace)
	checkStargateReady(t, f, ctx, stargateKey)
	checkStargateK8cStatusReady(t, f, ctx, kcKey, dcKey)
	checkContainerPresence(t, ctx, f, stargateDeploymentKey, getPodTemplateSpecForDeployment, stargate.VectorContainerName)
	checkVectorAgentConfigMapPresence(t, ctx, f, dcKey, stargate.VectorAgentConfigMapName)

	t.Log("check that if Stargate is deleted directly it gets re-created")
	apistargate := &stargateapi.Stargate{}
	err = f.Client.Get(ctx, stargateKey.NamespacedName, apistargate)
	require.NoError(err, "failed to get Stargate in namespace %s", namespace)
	err = f.Client.Delete(ctx, apistargate)
	require.NoError(err, "failed to delete Stargate in namespace %s", namespace)
	checkStargateReady(t, f, ctx, stargateKey)

	checkContainerPresence(t, ctx, f, stargateDeploymentKey, getPodTemplateSpecForDeployment, stargate.VectorContainerName)
	checkVectorAgentConfigMapPresence(t, ctx, f, dcKey, stargate.VectorAgentConfigMapName)

	t.Log("delete Stargate in k8ssandracluster resource")
	err = f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)
	dcGeneration := k8ssandra.Status.Datacenters["dc1"].Cassandra.ObservedGeneration
	patch := client.MergeFromWithOptions(k8ssandra.DeepCopy(), client.MergeFromWithOptimisticLock{})
	stargateTemplate := k8ssandra.Spec.Cassandra.Datacenters[0].Stargate
	k8ssandra.Spec.Cassandra.Datacenters[0].Stargate = nil
	err = f.Client.Patch(ctx, k8ssandra, patch)
	require.NoError(err, "failed to patch K8ssandraCluster in namespace %s", namespace)

	t.Log("check Stargate deleted")
	require.Eventually(func() bool {
		apistargate := &stargateapi.Stargate{}
		err := f.Client.Get(ctx, stargateKey.NamespacedName, apistargate)
		if err == nil || !errors.IsNotFound(err) {
			return false
		}
		k8ssandra := &api.K8ssandraCluster{}
		if err := f.Client.Get(ctx, kcKey, k8ssandra); err != nil {
			return false
		} else if kdcStatus, found := k8ssandra.Status.Datacenters[dcKey.Name]; !found {
			return false
		} else {
			return kdcStatus.Stargate == nil
		}
	}, polling.k8ssandraClusterStatus.timeout, polling.k8ssandraClusterStatus.interval)

	t.Log("check that Cassandra DC was not restarted")
	err = f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)
	require.Equal(dcGeneration, k8ssandra.Status.Datacenters["dc1"].Cassandra.ObservedGeneration)

	checkVectorConfigMapDeleted(t, ctx, f, dcKey, stargate.VectorAgentConfigMapName)

	t.Log("re-create Stargate in k8ssandracluster resource")
	err = f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)
	patch = client.MergeFromWithOptions(k8ssandra.DeepCopy(), client.MergeFromWithOptimisticLock{})
	k8ssandra.Spec.Cassandra.Datacenters[0].Stargate = stargateTemplate.DeepCopy()
	err = f.Client.Patch(ctx, k8ssandra, patch)
	require.NoError(err, "failed to patch K8ssandraCluster in operatorNamespace %s", namespace)
	checkStargateReady(t, f, ctx, stargateKey)

	checkContainerPresence(t, ctx, f, stargateDeploymentKey, getPodTemplateSpecForDeployment, stargate.VectorContainerName)
	checkVectorAgentConfigMapPresence(t, ctx, f, dcKey, stargate.VectorAgentConfigMapName)

	t.Log("check that Cassandra DC was not restarted")
	err = f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)
	require.Equal(dcGeneration, k8ssandra.Status.Datacenters["dc1"].Cassandra.ObservedGeneration)

	t.Log("retrieve database credentials")
	username, password, err := f.RetrieveDatabaseCredentials(ctx, f.DataPlaneContexts[0], namespace, k8ssandra.SanitizedName())
	require.NoError(err, "failed to retrieve database credentials")

	t.Log("deploying Stargate ingress routes in context", f.DataPlaneContexts[0])
	stargateRestHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateRest
	stargateGrpcHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateGrpc
	stargateCqlHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateCql
	f.DeployStargateIngresses(t, f.DataPlaneContexts[0], namespace, dcPrefix+"-stargate-service", stargateRestHostAndPort, stargateGrpcHostAndPort)
	defer f.UndeployAllIngresses(t, f.DataPlaneContexts[0], namespace)
	checkStargateApisReachable(t, ctx, f.DataPlaneContexts[0], namespace, dcPrefix, stargateRestHostAndPort, stargateGrpcHostAndPort, stargateCqlHostAndPort, username, password, false, f)

	replication := map[string]int{DcName(t, f, dcKey): 1}
	testStargateApis(t, f, ctx, f.DataPlaneContexts[0], namespace, dcPrefix, username, password, false, replication)

	t.Log("Disable Vector in k8ssandracluster and Stargate resources")
	err = f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)
	vectorPatch := client.MergeFromWithOptions(k8ssandra.DeepCopy(), client.MergeFromWithOptimisticLock{})
	k8ssandra.Spec.Cassandra.Telemetry.Vector.Enabled = pointer.Bool(false)
	k8ssandra.Spec.Cassandra.Datacenters[0].Stargate.Telemetry.Vector.Enabled = pointer.Bool(false)
	err = f.Client.Patch(ctx, k8ssandra, vectorPatch)
	require.NoError(err, "failed to patch K8ssandraCluster in namespace %s", namespace)
	checkDatacenterReady(t, ctx, dcKey, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dcKey.Name)
	// Check that Cassandra Vector's configmap is deleted
	checkVectorConfigMapDeleted(t, ctx, f, dcKey, telemetry.VectorAgentConfigMapName)
	// Check that Stargate Vector's configmap is deleted
	checkStargateReady(t, f, ctx, stargateKey)
	checkStargateK8cStatusReady(t, f, ctx, kcKey, dcKey)
	checkContainerDeleted(t, ctx, f, stargateDeploymentKey, getPodTemplateSpecForDeployment, stargate.VectorContainerName)
	checkVectorConfigMapDeleted(t, ctx, f, dcKey, stargate.VectorAgentConfigMapName)
}

// createSingleDatacenterClusterWithUpgrade creates a K8ssandraCluster with one CassandraDatacenter
// and one Stargate node that are deployed in the local cluster, using an older version of K8ssandraOperator.
// Then it performs an upgrade to the latest version and checks that the cluster is ready and functional.
func createSingleDatacenterClusterWithUpgrade(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {
	require := require.New(t)

	t.Log("check that the K8ssandraCluster was created")
	k8ssandra := &api.K8ssandraCluster{}
	kcKey := types.NamespacedName{Namespace: namespace, Name: "test"}
	err := f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)

	dcKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc1"}}
	checkDatacenterReady(t, ctx, dcKey, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dcKey.Name)
	dcPrefix := DcPrefix(t, f, dcKey)

	stargateKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: dcPrefix + "-stargate"}}
	checkStargateReady(t, f, ctx, stargateKey)
	checkStargateK8cStatusReady(t, f, ctx, kcKey, dcKey)

	// Save the Stargate deployment resource hash to verify if it was modified by the upgrade.
	// It'll allow to wait for the pod to be successfully upgraded before performing the Stargate API tests.
	stargateDeploymentKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: dcPrefix + "-default-stargate-deployment"}}
	initialStargateResourceHash := GetStargateResourceHash(t, f, ctx, stargateDeploymentKey)
	initialStargatePodNames := GetStargatePodNames(t, f, ctx, stargateDeploymentKey)
	require.Len(initialStargatePodNames, 1, "expected 1 Stargate pod in namespace %s", namespace)

	t.Log("retrieve database credentials")
	username, password, err := f.RetrieveDatabaseCredentials(ctx, f.DataPlaneContexts[0], namespace, k8ssandra.SanitizedName())
	require.NoError(err, "failed to retrieve database credentials")

	t.Log("deploying Stargate ingress routes in context", f.DataPlaneContexts[0])
	stargateRestHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateRest
	stargateGrpcHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateGrpc
	stargateCqlHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateCql
	f.DeployStargateIngresses(t, f.DataPlaneContexts[0], namespace, dcPrefix+"-stargate-service", stargateRestHostAndPort, stargateGrpcHostAndPort)
	defer f.UndeployAllIngresses(t, f.DataPlaneContexts[0], namespace)

	// Perform the upgrade
	err = upgradeToLatest(t, ctx, f, namespace)
	require.NoError(err, "failed to upgrade to latest version")

	// Wait for the Stargate deployment resource hash to change.
	// It'll allow to wait for the pod to be successfully upgraded before performing the Stargate API tests.
	// It's possible that this assertion will fail if the Stargate deployment resource hash is not changed.
	newStargateResourceHash := waitForStargateUpgrade(t, f, ctx, stargateDeploymentKey, initialStargateResourceHash)

	t.Logf("Stargate initial deployment resource hash: %s / Current hash: %s", initialStargateResourceHash, newStargateResourceHash)
	if initialStargateResourceHash != newStargateResourceHash {
		// Stargate deployment was modified after the upgrade, we need to wait for the new pod to be ready
		t.Log("Stargate deployment updated, waiting for new pod to be ready")
		require.Eventually(func() bool {
			newStargatePodNames := GetStargatePodNames(t, f, ctx, stargateDeploymentKey)
			return !utils.SliceContains(newStargatePodNames, initialStargatePodNames[0])
		}, polling.stargateReady.timeout, polling.stargateReady.interval)
	}

	checkStargateApisReachable(t, ctx, f.DataPlaneContexts[0], namespace, dcPrefix, stargateRestHostAndPort, stargateGrpcHostAndPort, stargateCqlHostAndPort, username, password, false, f)

	replication := map[string]int{DcName(t, f, dcKey): 1}
	testStargateApis(t, f, ctx, f.DataPlaneContexts[0], namespace, dcPrefix, username, password, false, replication)
}

// createSingleDatacenterCluster creates a K8ssandraCluster with one CassandraDatacenter
// and one Stargate node that are deployed in the local cluster.
func createSingleDatacenterClusterWithEncryption(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {
	require := require.New(t)
	require.NoError(f.CreateCassandraEncryptionStoresSecret(namespace), "Failed to create the encryption secrets")

	t.Log("check that the K8ssandraCluster was created")
	k8ssandra := &api.K8ssandraCluster{}
	kcKey := types.NamespacedName{Namespace: namespace, Name: "test"}
	err := f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)

	dcKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc1"}}
	checkDatacenterReady(t, ctx, dcKey, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dcKey.Name)
	dcPrefix := DcPrefix(t, f, dcKey)

	stargateKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: dcPrefix + "-stargate"}}
	checkStargateReady(t, f, ctx, stargateKey)
	checkStargateK8cStatusReady(t, f, ctx, kcKey, dcKey)

	t.Log("retrieve database credentials")
	username, password, err := f.RetrieveDatabaseCredentials(ctx, f.DataPlaneContexts[0], namespace, k8ssandra.SanitizedName())
	require.NoError(err, "failed to retrieve database credentials")

	t.Log("deploying Stargate ingress routes in context", f.DataPlaneContexts[0])
	stargateRestHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateRest
	stargateGrpcHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateGrpc
	stargateCqlHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateCql
	f.DeployStargateIngresses(t, f.DataPlaneContexts[0], namespace, dcPrefix+"-stargate-service", stargateRestHostAndPort, stargateGrpcHostAndPort)
	defer f.UndeployAllIngresses(t, f.DataPlaneContexts[0], namespace)
	checkStargateApisReachable(t, ctx, f.DataPlaneContexts[0], namespace, dcPrefix, stargateRestHostAndPort, stargateGrpcHostAndPort, stargateCqlHostAndPort, username, password, true, f)

	replication := map[string]int{DcName(t, f, dcKey): 1}
	testStargateApis(t, f, ctx, f.DataPlaneContexts[0], namespace, dcPrefix, username, password, true, replication)
}

// createStargateAndDatacenter creates a CassandraDatacenter with 3 nodes, one per rack. It also creates 1 or 3 Stargate
// nodes, one per rack, all deployed in the local cluster. Note that no K8ssandraCluster object is created.
func createStargateAndDatacenter(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {

	dcKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc1"}}
	checkDatacenterReady(t, ctx, dcKey, f)
	dcPrefix := DcPrefix(t, f, dcKey)
	stargateKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "s1"}}
	checkStargateReady(t, f, ctx, stargateKey)

	t.Log("retrieve database credentials")
	username, password, err := f.RetrieveDatabaseCredentials(ctx, f.DataPlaneContexts[0], namespace, "test")
	require.NoError(t, err, "failed to retrieve database credentials")

	t.Log("deploying Stargate ingress routes in context", f.DataPlaneContexts[0])
	stargateRestHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateRest
	stargateGrpcHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateGrpc
	stargateCqlHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateCql
	f.DeployStargateIngresses(t, f.DataPlaneContexts[0], namespace, dcPrefix+"-stargate-service", stargateRestHostAndPort, stargateGrpcHostAndPort)
	defer f.UndeployAllIngresses(t, f.DataPlaneContexts[0], namespace)
	checkStargateApisReachable(t, ctx, f.DataPlaneContexts[0], namespace, dcPrefix, stargateRestHostAndPort, stargateGrpcHostAndPort, stargateCqlHostAndPort, username, password, false, f)

	replication := map[string]int{DcName(t, f, dcKey): 3}
	testStargateApis(t, f, ctx, f.DataPlaneContexts[0], namespace, dcPrefix, username, password, false, replication)
}

// createMultiDatacenterCluster creates a K8ssandraCluster with two CassandraDatacenters,
// one running locally and the other running in a remote cluster.
func createMultiDatacenterCluster(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {
	require := require.New(t)

	t.Log("check that the K8ssandraCluster was created")
	k8ssandra := &api.K8ssandraCluster{}
	kcKey := client.ObjectKey{Namespace: namespace, Name: "test"}
	err := f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)

	dc1Key := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc1"}}
	checkDatacenterReady(t, ctx, dc1Key, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dc1Key.Name)

	dc2Key := framework.ClusterKey{K8sContext: f.DataPlaneContexts[1], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc2"}}
	checkDatacenterReady(t, ctx, dc2Key, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dc1Key.Name, dc2Key.Name)

	checkVectorAgentConfigMapPresence(t, ctx, f, dc1Key, telemetry.VectorAgentConfigMapName)
	checkVectorAgentConfigMapPresence(t, ctx, f, dc2Key, telemetry.VectorAgentConfigMapName)

	t.Log("retrieve database credentials")
	username, password, err := f.RetrieveDatabaseCredentials(ctx, f.DataPlaneContexts[0], namespace, k8ssandra.SanitizedName())
	require.NoError(err, "failed to retrieve database credentials")

	t.Log("check that nodes in dc1 see nodes in dc2")
	pod := DcPrefix(t, f, dc1Key) + "-rack1-sts-0"
	count := 4
	checkNodeToolStatus(t, f, f.DataPlaneContexts[0], namespace, pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(t, err, "timed out waiting for nodetool status check against "+pod)

	t.Log("check nodes in dc2 see nodes in dc1")
	pod = DcPrefix(t, f, dc2Key) + "-rack1-sts-0"
	checkNodeToolStatus(t, f, f.DataPlaneContexts[1], namespace, pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(t, err, "timed out waiting for nodetool status check against "+pod)
}

// createMultiDatacenterClusterDifferentTopologies creates a K8ssandraCluster with two CassandraDatacenters of
// different sizes.
func createMultiDatacenterClusterDifferentTopologies(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {
	require := require.New(t)

	t.Log("check that the K8ssandraCluster was created")
	kc := &api.K8ssandraCluster{}
	kcKey := client.ObjectKey{Namespace: namespace, Name: "test"}
	err := f.Client.Get(ctx, kcKey, kc)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)

	dc1Key := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc1"}}
	checkDatacenterReady(t, ctx, dc1Key, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dc1Key.Name)

	dc2Key := framework.ClusterKey{K8sContext: f.DataPlaneContexts[1], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc2"}}
	checkDatacenterReady(t, ctx, dc2Key, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dc1Key.Name, dc2Key.Name)

	t.Log("retrieve database credentials")
	username, password, err := f.RetrieveDatabaseCredentials(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName())
	require.NoError(err, "failed to retrieve database credentials")

	t.Log("check that nodes in dc1 see nodes in dc2")
	pod := DcPrefix(t, f, dc1Key) + "-default-sts-0"
	count := 3
	checkNodeToolStatus(t, f, f.DataPlaneContexts[0], namespace, pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(t, err, "timed out waiting for nodetool status check against "+pod)

	t.Log("check nodes in dc2 see nodes in dc1")
	pod = DcPrefix(t, f, dc2Key) + "-default-sts-0"
	checkNodeToolStatus(t, f, f.DataPlaneContexts[1], namespace, pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(t, err, "timed out waiting for nodetool status check against "+pod)

	replication := map[string]int{DcName(t, f, dc1Key): 2, DcName(t, f, dc2Key): 1}
	checkKeyspaceReplication(t, f, ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
		"system_auth", replication)
}

func addDcToCluster(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {
	require := require.New(t)
	assert := assert.New(t)

	t.Log("check that the K8ssandraCluster was created")
	kcKey := client.ObjectKey{Namespace: namespace, Name: "test"}
	kc := &api.K8ssandraCluster{}
	err := f.Client.Get(ctx, kcKey, kc)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)

	dc1Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[0],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      "dc1",
		},
	}
	checkDatacenterReady(t, ctx, dc1Key, f)

	sg1Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[0],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      DcPrefix(t, f, dc1Key) + "-stargate",
		},
	}
	checkStargateReady(t, f, ctx, sg1Key)

	reaper1Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[0],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      DcPrefix(t, f, dc1Key) + "-reaper",
		},
	}
	checkReaperReady(t, f, ctx, reaper1Key)

	dcSize := 2
	t.Log("create keyspaces")
	_, err = f.ExecuteCql(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
		fmt.Sprintf("CREATE KEYSPACE ks1 WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', '"+DcName(t, f, dc1Key)+"' : %d}", dcSize))
	require.NoError(err, "failed to create keyspace")

	_, err = f.ExecuteCql(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
		fmt.Sprintf("CREATE KEYSPACE ks2 WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', '"+DcName(t, f, dc1Key)+"' : %d}", dcSize))
	require.NoError(err, "failed to create keyspace")

	t.Log("add dc2 to cluster")

	require.Eventually(func() bool {
		kc := &api.K8ssandraCluster{}
		err = f.Client.Get(ctx, kcKey, kc)
		if err != nil {
			t.Logf("failed to add DC: failed to get K8ssandraCluster: %v", err)
			return false
		}

		kc.Spec.Cassandra.Datacenters = append(kc.Spec.Cassandra.Datacenters, api.CassandraDatacenterTemplate{
			Meta: api.EmbeddedObjectMeta{
				Name: "dc2",
			},
			K8sContext: f.DataPlaneContexts[1],
			Size:       int32(dcSize),
			DatacenterOptions: api.DatacenterOptions{
				DatacenterName: "real-dc2",
			},
		})
		annotations.AddAnnotation(kc, api.DcReplicationAnnotation, fmt.Sprintf("{\"real-dc2\": {\"ks1\": %d, \"ks2\": %d}}", dcSize, dcSize))

		err = f.Client.Update(ctx, kc)
		if err != nil {
			t.Logf("failed to add DC: %v", err)
			return false
		}

		return true
	}, 30*time.Second, 1*time.Second, "timed out waiting to add DC to K8ssandraCluster")

	dc2Key := framework.ClusterKey{K8sContext: f.DataPlaneContexts[1], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc2"}}
	checkDatacenterReady(t, ctx, dc2Key, f)

	rebuildDc2CassandraTask := &v1alpha1.CassandraTask{}
	rebuildDc2CassandraTaskKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[1], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc2-rebuild"}}

	require.Eventually(func() bool {
		err = f.Get(ctx, rebuildDc2CassandraTaskKey, rebuildDc2CassandraTask)
		if err != nil {
			t.Logf("failed to add DC: failed to get dc2-rebuild task: %v", err)
			return false
		}
		return rebuildDc2CassandraTask.Spec.CassandraTaskTemplate.Jobs[0].Arguments.SourceDatacenter == DcName(t, f, dc1Key)
	}, 3*time.Minute, 10*time.Second, "timed out waiting for CassandraTask to be created with the right source DC")

	t.Log("retrieve database credentials")
	username, password, err := f.RetrieveDatabaseCredentials(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName())
	require.NoError(err, "failed to retrieve database credentials")

	t.Log("check that nodes in dc1 see nodes in dc2")
	pod := DcPrefix(t, f, dc1Key) + "-default-sts-0"
	count := dcSize * 2
	checkNodeToolStatus(t, f, f.DataPlaneContexts[0], namespace, pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(err, "timed out waiting for nodetool status check against "+pod)

	t.Log("check nodes in dc2 see nodes in dc1")
	pod = DcPrefix(t, f, dc2Key) + "-default-sts-0"
	checkNodeToolStatus(t, f, f.DataPlaneContexts[1], namespace, pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(err, "timed out waiting for nodetool status check against "+pod)

	keyspaces := []string{"system_auth", stargate.AuthKeyspace, "ks1", "ks2"}
	for _, ks := range keyspaces {
		assert.Eventually(func() bool {
			output, err := f.ExecuteCql(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
				fmt.Sprintf("SELECT replication FROM system_schema.keyspaces WHERE keyspace_name = '%s'", ks))
			if err != nil {
				t.Logf("replication check for keyspace %s failed: %v", ks, err)
				return false
			}
			return strings.Contains(output, fmt.Sprintf("'%s': '%d'", DcName(t, f, dc1Key), dcSize)) && strings.Contains(output, fmt.Sprintf("'%s': '%d'", DcName(t, f, dc2Key), dcSize))
		}, 5*time.Minute, 15*time.Second, "failed to veify replication updated for keyspace %s", ks)
	}

	sg2Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[1],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      DcPrefix(t, f, dc2Key) + "-stargate",
		},
	}
	checkStargateReady(t, f, ctx, sg2Key)

	reaper2Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[1],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      DcPrefix(t, f, dc2Key) + "-reaper",
		},
	}
	checkReaperReady(t, f, ctx, reaper2Key)
}

func addDcToClusterSameDataplane(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {
	require := require.New(t)
	assert := assert.New(t)

	t.Log("check that the K8ssandraCluster was created")
	kcKey := client.ObjectKey{Namespace: namespace, Name: "test"}
	kc := &api.K8ssandraCluster{}
	err := f.Client.Get(ctx, kcKey, kc)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)

	dc1Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[0],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      "dc1",
		},
	}
	checkDatacenterReady(t, ctx, dc1Key, f)

	dcSize := 2
	t.Log("create keyspaces")
	_, err = f.ExecuteCql(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
		fmt.Sprintf("CREATE KEYSPACE ks1 WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', '%s' : %d}", DcName(t, f, dc1Key), dcSize))
	require.NoError(err, "failed to create keyspace")

	_, err = f.ExecuteCql(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
		fmt.Sprintf("CREATE KEYSPACE ks2 WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', '%s' : %d}", DcName(t, f, dc1Key), dcSize))
	require.NoError(err, "failed to create keyspace")

	t.Log("add dc2 to cluster")

	require.Eventually(func() bool {
		kc := &api.K8ssandraCluster{}
		err = f.Client.Get(ctx, kcKey, kc)
		if err != nil {
			t.Logf("failed to add DC: failed to get K8ssandraCluster: %v", err)
			return false
		}

		kc.Spec.Cassandra.Datacenters = append(kc.Spec.Cassandra.Datacenters, api.CassandraDatacenterTemplate{
			Meta: api.EmbeddedObjectMeta{
				Name:      "dc2",
				Namespace: "dc2",
			},
			K8sContext: f.DataPlaneContexts[0],
			Size:       int32(dcSize),
			DatacenterOptions: api.DatacenterOptions{
				DatacenterName: "real-dc2",
			},
		})
		annotations.AddAnnotation(kc, api.DcReplicationAnnotation, fmt.Sprintf("{\"real-dc2\": {\"ks1\": %d, \"ks2\": %d}}", dcSize, dcSize))

		err = f.Client.Update(ctx, kc)
		if err != nil {
			t.Logf("failed to add DC: %v", err)
			return false
		}

		return true
	}, 30*time.Second, 1*time.Second, "timed out waiting to add DC to K8ssandraCluster")

	dc2Key := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: "dc2", Name: "dc2"}}
	checkDatacenterReady(t, ctx, dc2Key, f)

	t.Log("retrieve database credentials")
	username, password, err := f.RetrieveDatabaseCredentials(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName())
	require.NoError(err, "failed to retrieve database credentials")

	t.Log("check that nodes in dc1 see nodes in dc2")
	pod := DcPrefix(t, f, dc1Key) + "-default-sts-0"
	count := dcSize * 2
	checkNodeToolStatus(t, f, f.DataPlaneContexts[0], namespace, pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(err, "timed out waiting for nodetool status check against "+pod)

	t.Log("check nodes in dc2 see nodes in dc1")
	pod = DcPrefix(t, f, dc2Key) + "-default-sts-0"
	checkNodeToolStatus(t, f, f.DataPlaneContexts[0], "dc2", pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(err, "timed out waiting for nodetool status check against "+pod)

	keyspaces := []string{"system_auth", "ks1", "ks2"}
	for _, ks := range keyspaces {
		assert.Eventually(func() bool {
			output, err := f.ExecuteCql(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
				fmt.Sprintf("SELECT replication FROM system_schema.keyspaces WHERE keyspace_name = '%s'", ks))
			if err != nil {
				t.Logf("replication check for keyspace %s failed: %v", ks, err)
				return false
			}
			t.Logf("replication check for keyspace %s: %s", ks, output)
			return strings.Contains(output, fmt.Sprintf("'%s': '%d'", DcName(t, f, dc1Key), dcSize)) && strings.Contains(output, fmt.Sprintf("'%s': '%d'", DcName(t, f, dc2Key), dcSize))
		}, 5*time.Minute, 15*time.Second, "failed to verify replication updated for keyspace %s", ks)
	}
}

func removeDcFromCluster(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {
	require := require.New(t)
	assert := assert.New(t)

	t.Log("check that the K8ssandraCluster was created")
	kcKey := client.ObjectKey{Namespace: namespace, Name: "test"}
	kc := &api.K8ssandraCluster{}
	err := f.Client.Get(ctx, kcKey, kc)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)

	dc1Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[0],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      "dc1",
		},
	}
	checkDatacenterReady(t, ctx, dc1Key, f)

	dc2Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[1],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      "dc2",
		},
	}
	checkDatacenterReady(t, ctx, dc2Key, f)

	t.Log("retrieve database credentials")
	username, password, err := f.RetrieveDatabaseCredentials(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName())
	require.NoError(err, "failed to retrieve database credentials")

	t.Log("check that nodes in dc1 see nodes in dc2")
	pod := DcPrefix(t, f, dc1Key) + "-default-sts-0"
	count := 2
	checkNodeToolStatus(t, f, f.DataPlaneContexts[0], namespace, pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(err, "timed out waiting for nodetool status check against "+pod)

	t.Log("check nodes in dc2 see nodes in dc1")
	pod = DcPrefix(t, f, dc2Key) + "-default-sts-0"
	checkNodeToolStatus(t, f, f.DataPlaneContexts[1], namespace, pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(err, "timed out waiting for nodetool status check against "+pod)

	sg1Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[0],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      DcPrefix(t, f, dc1Key) + "-stargate",
		},
	}
	checkStargateReady(t, f, ctx, sg1Key)

	reaper1Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[0],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      DcPrefix(t, f, dc1Key) + "-reaper",
		},
	}
	checkReaperReady(t, f, ctx, reaper1Key)

	sg2Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[1],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      DcPrefix(t, f, dc2Key) + "-stargate",
		},
	}
	checkStargateReady(t, f, ctx, sg2Key)

	reaper2Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[1],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      DcPrefix(t, f, dc2Key) + "-reaper",
		},
	}
	checkReaperReady(t, f, ctx, reaper2Key)

	t.Log("create keyspaces")
	_, err = f.ExecuteCql(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
		fmt.Sprintf("CREATE KEYSPACE ks1 WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', '%s': 1, '%s': 2}", DcName(t, f, dc1Key), DcName(t, f, dc2Key)))
	require.NoError(err, "failed to create keyspace")

	_, err = f.ExecuteCql(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
		fmt.Sprintf("CREATE KEYSPACE ks2 WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', '%s': 1, '%s': 2}", DcName(t, f, dc1Key), DcName(t, f, dc2Key)))
	require.NoError(err, "failed to create keyspace")
	dc2Name := DcName(t, f, dc2Key)

	t.Log("remove dc2 from cluster")
	err = f.Client.Get(ctx, kcKey, kc)
	require.NoError(err, "failed to get K8ssandraCluster %s", kcKey)
	kc.Spec.Cassandra.Datacenters = kc.Spec.Cassandra.Datacenters[:1]
	err = f.Client.Update(ctx, kc)
	require.NoError(err, "failed to remove dc2 from K8ssandraCluster spec")

	// Check that the datacenter isn't deleted and that we have an error for it in the status
	require.Eventually(func() bool {
		err = f.Client.Get(ctx, kcKey, kc)
		require.NoError(err, "failed to get K8ssandraCluster %s", kcKey)
		return kc.Status.Error != "None" && strings.Contains(kc.Status.Error, fmt.Sprintf("cannot decommission DC %s", dc2Name))
	}, 5*time.Minute, 5*time.Second, "timed out waiting for an error on dc2 removal")

	t.Log("alter keyspaces to remove replicas from DC2")
	_, err = f.ExecuteCql(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
		"ALTER KEYSPACE ks1 WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', '"+DcName(t, f, dc1Key)+"': 1}")
	require.NoError(err, "failed to alter keyspace")

	_, err = f.ExecuteCql(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
		"ALTER KEYSPACE ks2 WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', '"+DcName(t, f, dc1Key)+"': 1}")
	require.NoError(err, "failed to alter keyspace")

	f.AssertObjectDoesNotExist(ctx, t, dc2Key, &cassdcapi.CassandraDatacenter{}, 4*time.Minute, 5*time.Second)
	f.AssertObjectDoesNotExist(ctx, t, sg2Key, &stargateapi.Stargate{}, 1*time.Minute, 3*time.Second)
	f.AssertObjectDoesNotExist(ctx, t, reaper2Key, &reaperapi.Reaper{}, 1*time.Minute, 3*time.Second)

	keyspaces := []string{"system_auth", stargate.AuthKeyspace, reaperapi.DefaultKeyspace, "ks1", "ks2"}
	for _, ks := range keyspaces {
		assert.Eventually(func() bool {
			output, err := f.ExecuteCql(ctx, f.DataPlaneContexts[0], namespace, kc.SanitizedName(), DcPrefix(t, f, dc1Key)+"-default-sts-0",
				fmt.Sprintf("SELECT replication FROM system_schema.keyspaces WHERE keyspace_name = '%s'", ks))
			if err != nil {
				t.Logf("replication check for keyspace %s failed: %v", ks, err)
				return false
			}
			return strings.Contains(output, "'"+DcName(t, f, dc1Key)+"': '1'") && !strings.Contains(output, "'"+dc2Name+"': '1'")
		}, 1*time.Minute, 5*time.Second, "failed to verify replication updated for keyspace %s", ks)
	}

	t.Log("check that nodes in dc1 do not see nodes in dc2 anymore")
	pod = DcPrefix(t, f, dc1Key) + "-default-sts-0"
	count = 1
	checkNodeToolStatus(t, f, f.DataPlaneContexts[0], namespace, pod, count, 0, "-u", username, "-pw", password)
}

// removeLocalDcFromCluster removes a DC that was deployed in the same context as the control plane.
// This was broken in #642.
func removeLocalDcFromCluster(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {
	require := require.New(t)

	// Our test infrastructure creates a ClientConfig in the control plane pointing to itself. This would mask the issue
	// because ClientCache.getRemoteClients() will then contain a corresponding client, unlike a regular production
	// environment where ClientCache.GetLocalClient() is the only way to get to the control plane.
	// Remove the resource for a more realistic setup.
	t.Log("delete local ClientConfig")
	clientConfig := &v1beta1.ClientConfig{}
	err := f.Client.Get(ctx, types.NamespacedName{Namespace: namespace, Name: f.ControlPlaneContext}, clientConfig)
	require.NoError(err, "failed to get ClientConfig in namespace %s", namespace)
	err = f.Client.Delete(ctx, clientConfig)
	require.NoError(err, "failed to delete ClientConfig in namespace %s", namespace)

	t.Log("check that the K8ssandraCluster was created")
	kcKey := client.ObjectKey{Namespace: namespace, Name: "test"}
	kc := &api.K8ssandraCluster{}
	err = f.Client.Get(ctx, kcKey, kc)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)

	dc1Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[0],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      "dc1",
		},
	}
	checkDatacenterReady(t, ctx, dc1Key, f)

	dc2Key := framework.ClusterKey{
		K8sContext: f.DataPlaneContexts[0],
		NamespacedName: types.NamespacedName{
			Namespace: namespace,
			Name:      "dc2",
		},
	}
	checkDatacenterReady(t, ctx, dc2Key, f)

	t.Log("remove dc2 from cluster")
	err = f.Client.Get(ctx, kcKey, kc)
	require.NoError(err, "failed to get K8ssandraCluster %s", kcKey)
	kc.Spec.Cassandra.Datacenters = kc.Spec.Cassandra.Datacenters[:1]
	err = f.Client.Update(ctx, kc)
	require.NoError(err, "failed to remove dc2 from K8ssandraCluster spec")

	f.AssertObjectDoesNotExist(ctx, t, dc2Key, &cassdcapi.CassandraDatacenter{}, 4*time.Minute, 5*time.Second)
}

func checkStargateApisWithMultiDcCluster(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {
	require := require.New(t)
	require.NoError(f.CreateCassandraEncryptionStoresSecret(namespace), "Failed to create the encryption secrets")

	t.Log("check that the K8ssandraCluster was created")
	k8ssandra := &api.K8ssandraCluster{}
	kcKey := client.ObjectKey{Namespace: namespace, Name: "test"}
	err := f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)

	dc1Key := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc1"}}
	checkDatacenterReady(t, ctx, dc1Key, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dc1Key.Name)

	dc2Key := framework.ClusterKey{K8sContext: f.DataPlaneContexts[1], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc2"}}
	checkDatacenterReady(t, ctx, dc2Key, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dc1Key.Name, dc2Key.Name)

	dc1Prefix := DcPrefix(t, f, dc1Key)
	stargateKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: dc1Prefix + "-stargate"}}
	checkStargateReady(t, f, ctx, stargateKey)

	t.Logf("check k8ssandra cluster status updated for Stargate %s-stargate", dc1Prefix)
	require.Eventually(func() bool {
		k8ssandra := &api.K8ssandraCluster{}
		err := f.Client.Get(ctx, types.NamespacedName{Namespace: namespace, Name: "test"}, k8ssandra)
		if err != nil {
			return false
		}

		kdcStatus, found := k8ssandra.Status.Datacenters[dc1Key.Name]
		if !found {
			return false
		}
		if kdcStatus.Cassandra == nil {
			return false
		}

		if !cassandraDatacenterReady(kdcStatus.Cassandra) {
			return false
		}

		if kdcStatus.Stargate == nil {
			return false
		}
		return kdcStatus.Stargate.IsReady()
	}, polling.k8ssandraClusterStatus.timeout, polling.k8ssandraClusterStatus.interval)

	dc2Prefix := DcPrefix(t, f, dc2Key)
	stargateKey = framework.ClusterKey{K8sContext: f.DataPlaneContexts[1], NamespacedName: types.NamespacedName{Namespace: namespace, Name: dc2Prefix + "-stargate"}}
	checkStargateReady(t, f, ctx, stargateKey)

	t.Logf("check k8ssandra cluster status updated for Stargate %s-stargate", dc2Prefix)
	require.Eventually(func() bool {
		k8ssandra := &api.K8ssandraCluster{}
		err := f.Client.Get(ctx, types.NamespacedName{Namespace: namespace, Name: "test"}, k8ssandra)
		if err != nil {
			return false
		}

		kdcStatus, found := k8ssandra.Status.Datacenters[dc2Key.Name]
		if !found {
			return false
		}
		if kdcStatus.Cassandra == nil {
			return false
		}

		if !cassandraDatacenterReady(kdcStatus.Cassandra) {
			return false
		}

		if kdcStatus.Stargate == nil {
			return false
		}
		return kdcStatus.Stargate.IsReady()
	}, polling.k8ssandraClusterStatus.timeout, polling.k8ssandraClusterStatus.interval)

	t.Log("retrieve database credentials")
	username, password, err := f.RetrieveDatabaseCredentials(ctx, f.DataPlaneContexts[0], namespace, k8ssandra.SanitizedName())
	require.NoError(err, "failed to retrieve database credentials")

	t.Log("check that nodes in dc1 see nodes in dc2")
	pod := dc1Prefix + "-rack1-sts-0"
	count := 4
	checkNodeToolStatus(t, f, f.DataPlaneContexts[0], namespace, pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(t, err, "timed out waiting for nodetool status check against "+pod)

	t.Log("check nodes in dc2 see nodes in dc1")
	pod = dc2Prefix + "-rack1-sts-0"
	checkNodeToolStatus(t, f, f.DataPlaneContexts[1], namespace, pod, count, 0, "-u", username, "-pw", password)

	assert.NoError(t, err, "timed out waiting for nodetool status check against "+pod)

	t.Log("deploying Stargate ingress routes in context", f.DataPlaneContexts[0])
	stargateRestHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateRest
	stargateGrpcHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateGrpc
	stargateCqlHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateCql
	f.DeployStargateIngresses(t, f.DataPlaneContexts[0], namespace, dc1Prefix+"-stargate-service", stargateRestHostAndPort, stargateGrpcHostAndPort)
	defer f.UndeployAllIngresses(t, f.DataPlaneContexts[0], namespace)
	checkStargateApisReachable(t, ctx, f.DataPlaneContexts[0], namespace, dc1Prefix, stargateRestHostAndPort, stargateGrpcHostAndPort, stargateCqlHostAndPort, username, password, false, f)

	t.Log("deploying Stargate ingress routes in context", f.DataPlaneContexts[1])
	stargateRestHostAndPort = ingressConfigs[f.DataPlaneContexts[1]].StargateRest
	stargateGrpcHostAndPort = ingressConfigs[f.DataPlaneContexts[1]].StargateGrpc
	stargateCqlHostAndPort = ingressConfigs[f.DataPlaneContexts[1]].StargateCql
	f.DeployStargateIngresses(t, f.DataPlaneContexts[1], namespace, dc2Prefix+"-stargate-service", stargateRestHostAndPort, stargateGrpcHostAndPort)
	defer f.UndeployAllIngresses(t, f.DataPlaneContexts[1], namespace)
	checkStargateApisReachable(t, ctx, f.DataPlaneContexts[1], namespace, dc2Prefix, stargateRestHostAndPort, stargateGrpcHostAndPort, stargateCqlHostAndPort, username, password, false, f)

	replication := map[string]int{DcName(t, f, dc1Key): 1, DcName(t, f, dc2Key): 1}

	testStargateApis(t, f, ctx, f.DataPlaneContexts[0], namespace, dc1Prefix, username, password, false, replication)
	testStargateApis(t, f, ctx, f.DataPlaneContexts[1], namespace, dc2Prefix, username, password, false, replication)
}

func checkStargateApisWithMultiDcEncryptedCluster(t *testing.T, ctx context.Context, namespace string, f *framework.E2eFramework) {
	require := require.New(t)
	require.NoError(f.CreateCassandraEncryptionStoresSecret(namespace), "Failed to create the encryption secrets")

	t.Log("check that the K8ssandraCluster was created")
	k8ssandra := &api.K8ssandraCluster{}
	kcKey := client.ObjectKey{Namespace: namespace, Name: "test"}
	err := f.Client.Get(ctx, kcKey, k8ssandra)
	require.NoError(err, "failed to get K8ssandraCluster in namespace %s", namespace)

	dc1Key := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc1"}}
	checkDatacenterReady(t, ctx, dc1Key, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dc1Key.Name)

	dc2Key := framework.ClusterKey{K8sContext: f.DataPlaneContexts[1], NamespacedName: types.NamespacedName{Namespace: namespace, Name: "dc2"}}
	checkDatacenterReady(t, ctx, dc2Key, f)
	assertCassandraDatacenterK8cStatusReady(ctx, t, f, kcKey, dc1Key.Name, dc2Key.Name)
	t.Log("check k8ssandra cluster status")

	dc1Prefix := DcPrefix(t, f, dc1Key)
	stargateKey := framework.ClusterKey{K8sContext: f.DataPlaneContexts[0], NamespacedName: types.NamespacedName{Namespace: namespace, Name: dc1Prefix + "-stargate"}}
	checkStargateReady(t, f, ctx, stargateKey)

	t.Logf("check k8ssandra cluster status updated for Stargate %s-stargate", dc1Prefix)
	require.Eventually(func() bool {
		k8ssandra := &api.K8ssandraCluster{}
		err := f.Client.Get(ctx, types.NamespacedName{Namespace: namespace, Name: "test"}, k8ssandra)
		if err != nil {
			return false
		}

		kdcStatus, found := k8ssandra.Status.Datacenters[dc1Key.Name]
		if !found {
			return false
		}
		if kdcStatus.Cassandra == nil {
			return false
		}

		if !cassandraDatacenterReady(kdcStatus.Cassandra) {
			return false
		}

		if kdcStatus.Stargate == nil {
			return false
		}
		return kdcStatus.Stargate.IsReady()
	}, polling.k8ssandraClusterStatus.timeout, polling.k8ssandraClusterStatus.interval)

	dc2Prefix := DcPrefix(t, f, dc2Key)
	stargateKey = framework.ClusterKey{K8sContext: f.DataPlaneContexts[1], NamespacedName: types.NamespacedName{Namespace: namespace, Name: dc2Prefix + "-stargate"}}
	checkStargateReady(t, f, ctx, stargateKey)

	t.Logf("check k8ssandra cluster status updated for Stargate %s-stargate", dc2Prefix)
	require.Eventually(func() bool {
		k8ssandra := &api.K8ssandraCluster{}
		err := f.Client.Get(ctx, types.NamespacedName{Namespace: namespace, Name: "test"}, k8ssandra)
		if err != nil {
			return false
		}

		kdcStatus, found := k8ssandra.Status.Datacenters[dc2Key.Name]
		if !found {
			return false
		}
		if kdcStatus.Cassandra == nil {
			return false
		}

		if !cassandraDatacenterReady(kdcStatus.Cassandra) {
			return false
		}

		if kdcStatus.Stargate == nil {
			return false
		}
		return kdcStatus.Stargate.IsReady()
	}, polling.k8ssandraClusterStatus.timeout, polling.k8ssandraClusterStatus.interval)

	t.Log("retrieve database credentials")
	username, password, err := f.RetrieveDatabaseCredentials(ctx, f.DataPlaneContexts[0], namespace, k8ssandra.SanitizedName())
	require.NoError(err, "failed to retrieve database credentials")

	t.Log("deploying Stargate ingress routes in context", f.DataPlaneContexts[0])
	stargateRestHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateRest
	stargateGrpcHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateGrpc
	stargateCqlHostAndPort := ingressConfigs[f.DataPlaneContexts[0]].StargateCql
	f.DeployStargateIngresses(t, f.DataPlaneContexts[0], namespace, dc1Prefix+"-stargate-service", stargateRestHostAndPort, stargateGrpcHostAndPort)
	defer f.UndeployAllIngresses(t, f.DataPlaneContexts[0], namespace)
	checkStargateApisReachable(t, ctx, f.DataPlaneContexts[0], namespace, dc1Prefix, stargateRestHostAndPort, stargateGrpcHostAndPort, stargateCqlHostAndPort, username, password, true, f)

	t.Log("deploying Stargate ingress routes in context", f.DataPlaneContexts[1])
	stargateRestHostAndPort = ingressConfigs[f.DataPlaneContexts[1]].StargateRest
	stargateGrpcHostAndPort = ingressConfigs[f.DataPlaneContexts[1]].StargateGrpc
	stargateCqlHostAndPort = ingressConfigs[f.DataPlaneContexts[1]].StargateCql
	f.DeployStargateIngresses(t, f.DataPlaneContexts[1], namespace, dc2Prefix+"-stargate-service", stargateRestHostAndPort, stargateGrpcHostAndPort)
	defer f.UndeployAllIngresses(t, f.DataPlaneContexts[1], namespace)
	checkStargateApisReachable(t, ctx, f.DataPlaneContexts[1], namespace, dc2Prefix, stargateRestHostAndPort, stargateGrpcHostAndPort, stargateCqlHostAndPort, username, password, true, f)

	replication := map[string]int{DcName(t, f, dc1Key): 1, DcName(t, f, dc2Key): 1}

	testStargateApis(t, f, ctx, f.DataPlaneContexts[0], namespace, dc1Prefix, username, password, true, replication)
	testStargateApis(t, f, ctx, f.DataPlaneContexts[1], namespace, dc2Prefix, username, password, true, replication)
}

func checkDatacenterReady(t *testing.T, ctx context.Context, key framework.ClusterKey, f *framework.E2eFramework) {
	t.Logf("check that datacenter %s in cluster %s is ready", key.Name, key.K8sContext)
	withDatacenter := f.NewWithDatacenter(ctx, key)
	require.Eventually(t, withDatacenter(func(dc *cassdcapi.CassandraDatacenter) bool {
		status := dc.GetConditionStatus(cassdcapi.DatacenterReady)
		return status == corev1.ConditionTrue && dc.Status.CassandraOperatorProgress == cassdcapi.ProgressReady
	}), polling.datacenterReady.timeout, polling.datacenterReady.interval, fmt.Sprintf("timed out waiting for datacenter %s to become ready", key.Name))
}

func checkDatacenterUpdating(t *testing.T, ctx context.Context, key framework.ClusterKey, f *framework.E2eFramework) {
	t.Logf("check that datacenter %s in cluster %s is updating", key.Name, key.K8sContext)
	withDatacenter := f.NewWithDatacenter(ctx, key)
	require.Eventually(t, withDatacenter(func(dc *cassdcapi.CassandraDatacenter) bool {
		status := dc.GetConditionStatus(cassdcapi.DatacenterUpdating)
		return status == corev1.ConditionTrue && dc.Status.CassandraOperatorProgress == cassdcapi.ProgressUpdating
	}), polling.datacenterUpdating.timeout, polling.datacenterUpdating.interval, fmt.Sprintf("timed out waiting for datacenter %s to become updating", key.Name))
}

func checkDatacenterStopped(t *testing.T, ctx context.Context, key framework.ClusterKey, f *framework.E2eFramework) {
	t.Logf("check that datacenter %s in cluster %s is stopped", key.Name, key.K8sContext)
	withDatacenter := f.NewWithDatacenter(ctx, key)
	require.Eventually(t, withDatacenter(func(dc *cassdcapi.CassandraDatacenter) bool {
		status := dc.GetConditionStatus(cassdcapi.DatacenterStopped)
		return status == corev1.ConditionTrue && dc.Status.CassandraOperatorProgress == cassdcapi.ProgressReady
	}), polling.datacenterReady.timeout, polling.datacenterReady.interval, fmt.Sprintf("timed out waiting for datacenter %s to become stopped", key.Name))
}

func getCassandraDatacenterStatus(k8ssandra *api.K8ssandraCluster, dc string) *cassdcapi.CassandraDatacenterStatus {
	kdcStatus, found := k8ssandra.Status.Datacenters[dc]
	if !found {
		return nil
	}
	return kdcStatus.Cassandra
}

func cassandraDatacenterReady(status *cassdcapi.CassandraDatacenterStatus) bool {
	return status.GetConditionStatus(cassdcapi.DatacenterReady) == corev1.ConditionTrue &&
		status.CassandraOperatorProgress == cassdcapi.ProgressReady
}

func checkStargateReady(t *testing.T, f *framework.E2eFramework, ctx context.Context, stargateKey framework.ClusterKey) {
	t.Logf("check that Stargate %s in cluster %s is ready", stargateKey.Name, stargateKey.K8sContext)
	withStargate := f.NewWithStargate(ctx, stargateKey)
	require.Eventually(t, withStargate(func(stargate *stargateapi.Stargate) bool {
		return stargate.Status.IsReady()
	}), polling.stargateReady.timeout, polling.stargateReady.interval, "timed out waiting for Stargate %s to become ready", stargateKey.Name)
}

func checkStargateK8cStatusReady(
	t *testing.T,
	f *framework.E2eFramework,
	ctx context.Context,
	kcKey types.NamespacedName,
	dcKey framework.ClusterKey,
) {
	t.Log("check k8ssandra cluster status updated for Stargate")
	assert.Eventually(t, func() bool {
		k8ssandra := &api.K8ssandraCluster{}
		if err := f.Client.Get(ctx, kcKey, k8ssandra); err != nil {
			return false
		}
		kdcStatus, found := k8ssandra.Status.Datacenters[dcKey.Name]
		return found &&
			kdcStatus.Cassandra != nil &&
			cassandraDatacenterReady(kdcStatus.Cassandra) &&
			kdcStatus.Stargate != nil &&
			kdcStatus.Stargate.IsReady()
	}, polling.k8ssandraClusterStatus.timeout, polling.k8ssandraClusterStatus.interval, "timed out waiting for K8ssandraCluster status to get updated")
}

// checkNodeToolStatus polls until nodetool status reports the expected number of Up/Normal and Down/Normal nodes.
func checkNodeToolStatus(
	t *testing.T,
	f *framework.E2eFramework,
	k8sContext, namespace, pod string,
	countUN, countDN int,
	additionalArgs ...string,
) {
	require.Eventually(
		t,
		func() bool {
			actualUN, actualDN, err := f.GetNodeToolStatus(k8sContext, namespace, pod, additionalArgs...)
			return err == nil && actualUN == countUN && actualDN == countDN
		},
		polling.nodetoolStatus.timeout,
		polling.nodetoolStatus.interval,
		"timed out waiting for nodetool status to reach count UN %v and DN %v",
		countUN, countDN,
	)
}

// FIXME there is some overlap with E2eFramework.RetrieveDatabaseCredentials()
func retrieveCredentials(t *testing.T, f *framework.E2eFramework, ctx context.Context, secretKey framework.ClusterKey) (string, string) {
	secret := getSecret(t, f, ctx, secretKey)
	return string(secret.Data["username"]), string(secret.Data["password"])
}

func getSecret(t *testing.T, f *framework.E2eFramework, ctx context.Context, secretKey framework.ClusterKey) *corev1.Secret {
	secret := &corev1.Secret{}
	require.Eventually(
		t,
		func() bool {
			return f.Get(ctx, secretKey, secret) == nil
		},
		time.Minute,
		time.Second,
		"secret %s does not exist",
		secretKey,
	)
	return secret
}

func checkKeyspaceExists(
	t *testing.T,
	f *framework.E2eFramework,
	ctx context.Context,
	k8sContext, namespace, clusterName, pod, keyspace string,
) {
	assert.Eventually(t, func() bool {
		keyspaces, err := f.ExecuteCql(ctx, k8sContext, namespace, clusterName, pod, "describe keyspaces")
		if err != nil {
			t.Logf("failed to describe keyspaces: %v", err)
			return false
		}
		return strings.Contains(keyspaces, keyspace)
	}, 1*time.Minute, 3*time.Second)
}

func checkKeyspaceReplication(
	t *testing.T,
	f *framework.E2eFramework,
	ctx context.Context,
	k8sContext, namespace, clusterName, pod, keyspace string,
	replication map[string]int,
) {
	assert.Eventually(t, func() bool {
		desc, err := f.ExecuteCql(ctx, k8sContext, namespace, clusterName, pod, "describe keyspace "+keyspace)
		if err != nil {
			t.Logf("failed to desctibe keyspace %s: %v", keyspace, err)
			return false
		}
		for dc, rf := range replication {
			rfStr := fmt.Sprintf("'%v': '%v'", dc, rf)
			if !strings.Contains(desc, rfStr) {
				t.Logf("Keyspace %s replication does not contain: %v", keyspace, rfStr)
				return false
			}
		}
		return true
	}, 1*time.Minute, 3*time.Second)
}

// assertCassandraDatacenterK8cStatusReady polls the K8ssandraCluster object, checking its status to
// verify the CassandraDatacenter specified by dcName is ready.
func assertCassandraDatacenterK8cStatusReady(
	ctx context.Context,
	t *testing.T,
	f *framework.E2eFramework,
	kcKey client.ObjectKey,
	dcNames ...string,
) {
	t.Logf("check K8ssandraCluster status for CassandraDatacenters %v ready", dcNames)
	assert.Eventually(t, func() bool {
		kc := &api.K8ssandraCluster{}
		err := f.Client.Get(ctx, kcKey, kc)
		if err != nil {
			return false
		}

		for _, dcName := range dcNames {
			dcStatus := getCassandraDatacenterStatus(kc, dcName)
			if dcStatus == nil || !cassandraDatacenterReady(dcStatus) {
				return false
			}
		}
		return true
	}, polling.k8ssandraClusterStatus.timeout, polling.k8ssandraClusterStatus.interval, "timed out waiting for K8ssandraCluster status to get updated")
}

func checkStargateApisReachable(
	t *testing.T,
	ctx context.Context,
	k8sContext, namespace, clusterName string,
	stargateRestHostAndPort, stargateGrpcHostAndPort, stargateCqlHostAndPort framework.HostAndPort,
	username, password string,
	cqlSsl bool,
	f *framework.E2eFramework,
) {
	timeout := 2 * time.Minute
	interval := 1 * time.Second
	stargateHttp := fmt.Sprintf("http://%v/v1/auth", stargateRestHostAndPort)
	require.Eventually(t, func() bool {
		body := map[string]string{"username": username, "password": password}
		request := resty.NewRequest().
			SetHeader("Content-Type", "application/json").
			SetBody(body)
		response, err := request.Post(stargateHttp)
		if username != "" {
			return err == nil && response.StatusCode() == http.StatusCreated
		} else {
			return err == nil && response.StatusCode() == http.StatusBadRequest
		}
	}, timeout, interval, "Address is unreachable: %s", stargateHttp)
	// Stargate gRPC requires a token, we can't connect if we don't have credentials
	if username != "" {
		require.Eventually(t, func() bool {
			connection, err := f.GetStargateGrpcConnection(ctx, k8sContext, namespace, clusterName, username, password, stargateGrpcHostAndPort, stargateRestHostAndPort)
			if err != nil {
				t.Log(err)
				return false
			}
			_ = connection.Close()
			return true
		}, timeout, interval, "Address is unreachable: %s", stargateGrpcHostAndPort)
	}
	require.Eventually(t, func() bool {
		connection, err := f.GetCqlClientConnection(ctx, k8sContext, namespace, stargateCqlHostAndPort, username, password, cqlSsl)
		if err != nil {
			return false
		}
		_ = connection.Close()
		return true
	}, timeout, interval, "Address is unreachable: %s", stargateCqlHostAndPort)
}

func checkReaperApiReachable(t *testing.T, ctx context.Context, reaperHostAndPort framework.HostAndPort) {
	timeout := 2 * time.Minute
	interval := 1 * time.Second
	reaperHttp := fmt.Sprintf("http://%s", reaperHostAndPort)
	require.Eventually(t, func() bool {
		reaperURL, _ := url.Parse(reaperHttp)
		reaperClient := reaperclient.NewClient(reaperURL)
		up, err := reaperClient.IsReaperUp(ctx)
		return up && err == nil
	}, timeout, interval, "Address is unreachable: %s", reaperHttp)
}

func configureZeroLog() {
	zerolog.SetGlobalLevel(zerolog.WarnLevel)
	log.Logger = log.Output(zerolog.ConsoleWriter{
		Out:        os.Stderr,
		TimeFormat: zerolog.TimeFormatUnix,
	})
}

func DcPrefix(
	t *testing.T,
	f *framework.E2eFramework,
	dcKey framework.ClusterKey) string {
	// Get the cassdc object
	cassdc := &cassdcapi.CassandraDatacenter{}
	err := f.Get(context.Background(), dcKey, cassdc)
	require.NoError(t, err)
	return framework.CleanupForKubernetes(fmt.Sprintf("%s-%s", cassdc.Spec.ClusterName, cassdc.DatacenterName()))
}

func DcClusterName(
	t *testing.T,
	f *framework.E2eFramework,
	dcKey framework.ClusterKey) string {
	// Get the cassdc object
	cassdc := &cassdcapi.CassandraDatacenter{}
	err := f.Get(context.Background(), dcKey, cassdc)
	require.NoError(t, err)
	return cassdc.Spec.ClusterName
}

func DcName(
	t *testing.T,
	f *framework.E2eFramework,
	dcKey framework.ClusterKey) string {
	// Get the cassdc object
	cassdc := &cassdcapi.CassandraDatacenter{}
	err := f.Get(context.Background(), dcKey, cassdc)
	require.NoError(t, err)
	return cassdc.DatacenterName()
}

func waitForStargateUpgrade(t *testing.T, f *framework.E2eFramework, ctx context.Context, stargateDeploymentKey framework.ClusterKey, initialStargateResourceHash string) string {
	stargateChan := make(chan string)
	go func() {
		for {
			stargateDeploymentResourceHash := GetStargateResourceHash(t, f, ctx, stargateDeploymentKey)
			if stargateDeploymentResourceHash != initialStargateResourceHash {
				stargateChan <- stargateDeploymentResourceHash
				return
			}
			time.Sleep(time.Second)
		}
	}()

	stargateUpgradeTimeout := time.After(5 * time.Minute)
	for {
		select {
		case newStargateResourceHash := <-stargateChan:
			t.Logf("Stargate deployment resource hash changed to %s", newStargateResourceHash)
			return newStargateResourceHash
		case <-stargateUpgradeTimeout:
			t.Log("Stargate deployment resource hash did not change")
			return initialStargateResourceHash
		}
	}
}

func checkMetricsFiltersAbsence(t *testing.T, ctx context.Context, f *framework.E2eFramework, dcKey framework.ClusterKey) error {
	t.Logf("check that metric filters are absent on dc %s in cluster %s", dcKey.Name, dcKey.K8sContext)
	cassdc := &cassdcapi.CassandraDatacenter{}
	err := f.Get(ctx, dcKey, cassdc)
	if err != nil {
		return err
	}

	if containerIndex, containerFound := cassandra.FindContainer(cassdc.Spec.PodTemplateSpec, "cassandra"); containerFound {
		envVariables := cassdc.Spec.PodTemplateSpec.Spec.Containers[containerIndex].Env
		// METRIC_FILTERS env variable should be absent as MCAC is disabled
		require.Nil(t, utils.FindEnvVar(envVariables, "METRIC_FILTERS"), "METRIC_FILTERS env variable was found in cassandra container")
	} else {
		return fmt.Errorf("cannot find cassandra container in pod template spec")
	}
	return nil
}

func checkInjectedContainersPresence(t *testing.T, ctx context.Context, f *framework.E2eFramework, dcKey framework.ClusterKey) error {
	t.Logf("check that containers were injected in %s cass pods in cluster %s", dcKey.Name, dcKey.K8sContext)
	cassdc := &cassdcapi.CassandraDatacenter{}
	err := f.Get(ctx, dcKey, cassdc)
	if err != nil {
		return err
	}

	if containerIndex, containerFound := cassandra.FindContainer(cassdc.Spec.PodTemplateSpec, "busybox"); containerFound {
		require.Equal(t, 2, containerIndex, "busybox container should be the third container in cassandra pod")
	} else {
		return fmt.Errorf("cannot find busybox injected container in pod template spec")
	}

	if initContainerIndex, initContainerFound := cassandra.FindInitContainer(cassdc.Spec.PodTemplateSpec, "init-busybox"); initContainerFound {
		require.Equal(t, 1, initContainerIndex, "init-busybox container should be the second init container in cassandra pod")
	} else {
		return fmt.Errorf("cannot find init-busybox injected container in pod template spec")
	}
	return nil
}

func checkInjectedVolumePresence(t *testing.T, ctx context.Context, f *framework.E2eFramework, dcKey framework.ClusterKey, nbVolumes int) error {
	t.Logf("check that volumes were injected in %s cass pods in cluster %s", dcKey.Name, dcKey.K8sContext)
	cassdc := &cassdcapi.CassandraDatacenter{}
	err := f.Get(ctx, dcKey, cassdc)
	if err != nil {
		return err
	}

	require.Equal(t, nbVolumes, len(cassdc.Spec.StorageConfig.AdditionalVolumes), "expected a different number of additional volume")
	require.Equal(t, "/var/lib/extra", cassdc.Spec.StorageConfig.AdditionalVolumes[0].MountPath, "expected busybox-vol mount path")

	_, found := cassandra.FindVolume(cassdc.Spec.PodTemplateSpec, "busybox-vol")
	require.True(t, found, "busybox-vol volume not found in cassandra pod")

	if containerIndex, containerFound := cassandra.FindContainer(cassdc.Spec.PodTemplateSpec, "busybox"); containerFound {
		volumeMount := cassandra.FindVolumeMount(&cassdc.Spec.PodTemplateSpec.Spec.Containers[containerIndex], "busybox-vol")
		require.NotNil(t, volumeMount, "busybox-vol volume mount not found in busybox container")
		require.Equal(t, "/var/lib/busybox", volumeMount.MountPath, "expected busybox-vol mount path")
	} else {
		return fmt.Errorf("cannot find busybox injected container in pod template spec")
	}

	if initContainerIndex, initContainerFound := cassandra.FindInitContainer(cassdc.Spec.PodTemplateSpec, "init-busybox"); initContainerFound {
		volumeMount := cassandra.FindVolumeMount(&cassdc.Spec.PodTemplateSpec.Spec.InitContainers[initContainerIndex], "busybox-vol")
		require.NotNil(t, volumeMount, "busybox-vol volume mount not found in init-busybox container")
		require.Equal(t, "/var/lib/busybox", volumeMount.MountPath, "expected busybox-vol mount path")
	} else {
		return fmt.Errorf("cannot find busybox injected container in pod template spec")
	}

	cassandraPods, err := f.GetCassandraDatacenterPods(t, ctx, dcKey, cassdc.DatacenterName())
	require.NoError(t, err, "failed listing Cassandra pods")
	cassandraIndex, cassandraFound := findContainerInPod(t, cassandraPods[0], "cassandra")
	require.True(t, cassandraFound, "cannot find cassandra container in cassandra pod")
	volumeMount := cassandra.FindVolumeMount(&cassandraPods[0].Spec.Containers[cassandraIndex], "sts-extra-vol")
	require.NotNil(t, volumeMount, "sts-extra-vol volume mount not found in cassandra container")
	require.Equal(t, "/var/lib/extra", volumeMount.MountPath, "expected sts-extra-vol mount path")

	return nil
}

func findContainerInPod(t *testing.T, pod corev1.Pod, containerName string) (index int, found bool) {
	for i, container := range pod.Spec.Containers {
		t.Logf("checking container %s in pod %s", container.Name, pod.Name)
		if container.Name == containerName {
			return i, true
		}
	}
	return -1, false
}

func checkCassandraClusterName(t *testing.T, ctx context.Context, k8ssandra *api.K8ssandraCluster, dcKey framework.ClusterKey, f *framework.E2eFramework) {
	t.Logf("check that the cassdc object has the right overriden cluster name, without any modification: %s", k8ssandra.Spec.Cassandra.ClusterName)
	cassdc := &cassdcapi.CassandraDatacenter{}
	err := f.Get(ctx, dcKey, cassdc)
	require.NoError(t, err, "failed to get cassdc object")

	require.Equal(t, k8ssandra.Spec.Cassandra.ClusterName, cassdc.Spec.ClusterName, "cassdc cluster name is not the expected one")
}

func checkVectorConfigMapDeleted(t *testing.T, ctx context.Context, f *framework.E2eFramework, dcKey framework.ClusterKey, configMapNameFunc func(clusterName string, dcName string) string) {
	configMapName := types.NamespacedName{
		Namespace: dcKey.Namespace,
		Name:      cassdcapi.CleanupForKubernetes(configMapNameFunc(DcClusterName(t, f, dcKey), dcKey.Name)),
	}
	t.Logf("check that Vector ConfigMap %s is deleted in cluster %s", configMapName.Name, dcKey.K8sContext)
	configMapKey := framework.ClusterKey{
		NamespacedName: configMapName,
		K8sContext:     dcKey.K8sContext,
	}
	require.Eventually(t, func() bool {
		// Check that vector's configmap is deleted
		cm := &corev1.ConfigMap{}
		err := f.Get(ctx, configMapKey, cm)
		if err != nil && errors.IsNotFound(err) {
			return true
		}
		return false
	}, polling.k8ssandraClusterStatus.timeout, polling.k8ssandraClusterStatus.interval, "Vector configmap was not deleted")

}

func getPodTemplateSpecForDeployment(t *testing.T, ctx context.Context, f *framework.E2eFramework, deploymentKey framework.ClusterKey) *corev1.PodTemplateSpec {
	sg := &appsv1.Deployment{}
	require.NoError(t, f.Get(ctx, deploymentKey, sg), "failed to get Deployment")

	return &sg.Spec.Template
}

func checkContainerPresence(t *testing.T, ctx context.Context, f *framework.E2eFramework, podKey framework.ClusterKey, specFunction func(t *testing.T, ctx context.Context, f *framework.E2eFramework, dcKey framework.ClusterKey) *corev1.PodTemplateSpec, containerName string) {
	t.Logf("check that %s contains Container named %s", podKey.Name, containerName)
	podTempSpec := specFunction(t, ctx, f, podKey)
	_, containerFound := cassandra.FindContainer(podTempSpec, containerName)
	require.True(t, containerFound, "cannot find Container in pod template spec")
}

func checkContainerDeleted(t *testing.T, ctx context.Context, f *framework.E2eFramework, podKey framework.ClusterKey, specFunction func(t *testing.T, ctx context.Context, f *framework.E2eFramework, dcKey framework.ClusterKey) *corev1.PodTemplateSpec, containerName string) {
	t.Logf("check that %s does not have a Container named %s", podKey.Name, containerName)
	podTempSpec := specFunction(t, ctx, f, podKey)
	_, containerFound := cassandra.FindContainer(podTempSpec, containerName)
	require.False(t, containerFound, "Found Container in pod template spec")
}

func checkVectorAgentConfigMapPresence(t *testing.T, ctx context.Context, f *framework.E2eFramework, dcKey framework.ClusterKey, configMapNameFunc func(clusterName string, dcName string) string) {
	configMapName := types.NamespacedName{
		Namespace: dcKey.Namespace,
		Name:      cassdcapi.CleanupForKubernetes(configMapNameFunc(DcClusterName(t, f, dcKey), DcName(t, f, dcKey))),
	}
	t.Logf("check that Vector agent config map %v is present in cluster %s", configMapName, dcKey.K8sContext)
	configMapKey := framework.ClusterKey{
		NamespacedName: configMapName,
		K8sContext:     dcKey.K8sContext,
	}
	require.Eventually(t, func() bool {
		cm := &corev1.ConfigMap{}
		err := f.Get(ctx, configMapKey, cm)
		return err == nil
	}, polling.k8ssandraClusterStatus.timeout, polling.k8ssandraClusterStatus.interval, "Vector configmap was not found")

}

func CheckLabelsAnnotationsCreated(dcKey framework.ClusterKey, t *testing.T, ctx context.Context, f *framework.E2eFramework) error {
	cassDC := &cassdcapi.CassandraDatacenter{}
	t.Logf("check that Annotations and Labels fields are propagated to cassDC %v is present in cluster %s", cassDC.Name, dcKey.K8sContext)
	err := f.Get(ctx, dcKey, cassDC)
	if err != nil {
		return err
	}
	assert.True(t, cassDC.Spec.AdditionalLabels["aLabelKeyClusterLevel"] == "aLabelValueClusterLevel")
	assert.True(t, cassDC.Spec.AdditionalLabels["aLabelKeyDcLevel"] == "aLabelValueDcLevel")
	assert.True(t, cassDC.Spec.AdditionalAnnotations["anAnnotationKeyDcLevel"] == "anAnnotationValueDCLevel")
	assert.True(t, cassDC.Spec.AdditionalAnnotations["anAnnotationKeyClusterLevel"] == "anAnnotationValueClusterLevel")
	return nil
}
